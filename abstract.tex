Activities like one-on-one video chatting and video conferencing with multiple participants are more prevalent than ever today as we continue to battle our way through the pandemic. Bringing a 3D feel to video chat has always been a hot topic in Vision and Graphics communities. In this thesis, we have employed novel view synthesis in attempting to turn one-on-one video chatting into 3D. We have tuned the learning pipeline of Tucker and Snavely's~\cite{single_view_mpi} single-view view synthesis paper --- by retraining it on MannequinChallenge Dataset~\cite{li2019learning} --- to better predict a layered representation of the scene being viewed by either video chat participant at any given time. This intermediate representation --- called a Multiplane Image (MPI) --- may then be used to rerender the scene at an arbitrary viewpoint which, in our case, matches with the head pose of the watcher in the opposite, concurrent video frame. We discuss that our pipeline, when implemented in real-time, will allow both video chat participants to unravel occluded scene content and ``peer into" each other's dynamic video scenes to a certain extent as it would enable full parallax up to the baselines of small head rotations and/or translations. It is similar to a VR headset's ability to determine the position and orientation of the wearer's head in 3D space and render any scene in alignment with this estimated head pose. We have attempted to improve the performance of our retained model by extending MannequinChallenge with RealEstate10K Dataset~\cite{zhou2018stereo}, originally used by Tucker and Snavely. We present a quantitative and qualitative comparison of our model variants, taking Tucker and Snavely's model as baseline. We also describe our impactful dataset curation process among other aspects.