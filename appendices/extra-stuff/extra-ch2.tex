Although there has also been simultaneous progress in the field of free viewpoint TV, yet the technology/hardware is not up to the mark yet to warrant a mention in this section. They are similar to 360-degree YouTube videos that probably capture entire light fields pertaining to the scene, but can be viewed on much bigger screens.

``Display devices like VR headsets are functionally holographic --- as the user moves around they have to have new perspectives for each position that they are in; and in order to serve that you have to have something fundamentally holographic. Moreover we're also getting this on 2D screens: multiperspective panels. So it's not that far off before all your phones and computers and TVs are going to be multiperspective too, at which point 2D images are going to look kind of uninteresting by comparison."~\url{https://youtu.be/BXdKVisWAco}. This would be like comparing today's newspapers with the fictional wizarding world's newspaper, The Daily Prophet.

How Lytro cameras work; how do they capture light fields:~\url{https://www.youtube.com/watch?v=rEMP3XEgnws}.

The DeepView paper cover page video covering a broad range of topics from standard MPI generation to a more perfect MPI generation:~\url{https://augmentedperception.github.io/deepview/}. Similarly, the cover video for the earlier Local Light Field Fusion paper:~\url{https://github.com/Fyusion/LLFF}. 

Actually, the DeepView paper has a beautiful software to customized, visualize, and render any type of MPI! It's kind of like the state of the art MPI manipulator.~\url{https://augmentedperception.github.io/deepview/}.

Finally understood what scale-invariant view synthesis is!!! Both stereo MPI and single-view MPI use view synthesis for supervision, perhaps not unlike how papers that predict depth and optical flow use view synthesis for supervision. Whereas stereo MPI can outright use a third view as ground-truth, single-view MPI can also use a second view as ground truth with the caveat that since the scale of the scene depicted by the monocular view can only be arbitrary, they'd have to somehow make the entire training scale invariant, because making just the ground truth scale invariant doesn't make sense.

How does Tucker and Snavely map world points into source camera space?
\url{https://learnopengl.com/Getting-started/Coordinate-Systems}
\url{http://www.cse.psu.edu/~rtc12/CSE486/}
\url{cse.psu.edu/~rtc12/CSE486/lecture12.pdf}
\url{https://www.scratchapixel.com/lessons/3d-basic-rendering/computing-pixel-coordinates-of-3d-point/mathematics-computing-2d-coordinates-of-3d-points}

Other papers find a scale factor that minimizes a scale-dependent loss, and rely on there being a closed-form solution for this scale factor.
\url{https://cs.nyu.edu/~deigen/depth/}
\url{https://arxiv.org/abs/1612.02401}

Related work and presentations that can help with defense and write-up:
\url{https://cs.brown.edu/courses/cs129/2012/}
\url{https://www.youtube.com/watch?v=ZIHZ77y5pXY}
\url{https://www.youtube.com/watch?v=zem03fZWLrQ}
\url{https://www.youtube.com/watch?v=NqqbQJCqFvI}
\url{https://www.youtube.com/watch?v=p2w1DNkITI8}

Project Starline is a complete end to end symmetric communication system that gives people the experience of being with another person who is in fact far away. The system consists of a head-tracked autostereoscopic display, high-resolution 3D capture and rendering subsystems, and network transmission using compressed color and depth video streams. A 3D video of the participant is reconstructed at 60 frames per second from two camera viewpoints located above the display and one viewpoint located in a central middle wall. This middle wall also serves to hide the bottom edge of the display to avoid depth conflicts that would otherwise appear whenever the remote person's body at that edge extends beyond the plane of the display. This real-time 3D video and 3D audio data is then compressed and streamed bidirectionally over the internet to enable a synchronous conversation to take place between the two sides. The combined effect is that each participant can see and hear the other person as they truly are within a head box of roughly one meter cubed centered about 1.2 meters in font of the display. Key nonverbal cues such as eye contact and hand gestures are all intuitively conveyed. 

Project Starline references:
\url{https://en.wikipedia.org/wiki/Project_Starline}
\url{https://www.theverge.com/2021/5/18/22442336/google-project-starline-3d-video-chat-platform}
\url{wired.com/story/google-project-starline/}
\url{https://mashable.com/video/google-project-starline-3d-video-calls}

Fun little CBS Mornings TV news spot about Facebook Labs' 3D video calls thingiemajiggie:
\url{https://www.youtube.com/watch?v=1rRHMiwERWY}