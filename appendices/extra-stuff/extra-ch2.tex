Although there has also been simultaneous progress in the field of free viewpoint TV, yet the technology/hardware is not up to the mark yet to warrant a mention in this section. They are similar to 360-degree YouTube videos that probably capture entire light fields pertaining to the scene but can be viewed on much bigger screens.

``Display devices like VR headsets are functionally holographic --- as the user moves around they have to have new perspectives for each position that they are in; and in order to serve that you have to have something fundamentally holographic. Moreover we're also getting this on 2D screens: multiperspective panels. So it's not that far off before all your phones and computers and TVs are going to be multiperspective too, at which point 2D images are going to look kind of uninteresting by comparison."~\url{https://youtu.be/BXdKVisWAco}. This would be like comparing today's newspapers with the fictional wizarding world's newspaper, The Daily Prophet.

How Lytro cameras work; how do they capture light fields:~\url{https://www.youtube.com/watch?v=rEMP3XEgnws}.

The DeepView paper cover page video covering a broad range of topics from standard MPI generation to a more perfect MPI generation:~\url{https://augmentedperception.github.io/deepview/}. Similarly the cover video for the earlier Local Light Field Fusion paper:~\url{https://github.com/Fyusion/LLFF}. 

Actually, the DeepView paper has a beautiful software to customized, visualize, and render any type of MPI! It's kind of like the state of the art MPI manipulator.~\url{https://augmentedperception.github.io/deepview/}.

Finally understood what scale-invariant view synthesis is!!! Both stereo MPI and single-view MPI use view synthesis for supervision, perhaps not unlike how papers that predict depth and optical flow use view synthesis for supervision. Whereas stereo MPI can outright use a third view as ground-truth, single-view MPI can also use a second view as ground truth with the caveat that since the scale of the scene depicted by the monocular view can only be arbitrary, they'd have to somehow make the entire training scale invariant, because making just the ground truth scale invariant doesn't make sense.

How does Tucker and Snavely map world points into source camera space?
\url{https://learnopengl.com/Getting-started/Coordinate-Systems}
\url{http://www.cse.psu.edu/~rtc12/CSE486/}
\url{cse.psu.edu/~rtc12/CSE486/lecture12.pdf}
\url{https://www.scratchapixel.com/lessons/3d-basic-rendering/computing-pixel-coordinates-of-3d-point/mathematics-computing-2d-coordinates-of-3d-points}

Other papers find a scale factor that minimizes a scale-dependent loss, and rely on there being a closed-form solution for this scale factor.
\url{https://cs.nyu.edu/~deigen/depth/}
\url{https://arxiv.org/abs/1612.02401}

Related work and presentations that can help with defense or write-up:
\url{https://cs.brown.edu/courses/cs129/2012/}
\url{https://www.youtube.com/watch?v=ZIHZ77y5pXY}
\url{https://www.youtube.com/watch?v=zem03fZWLrQ}
\url{https://www.youtube.com/watch?v=NqqbQJCqFvI}
\url{https://www.youtube.com/watch?v=p2w1DNkITI8}
