final count 
re10k training 9095528 frames from 66861 videos 
manne training 0117811 frames from 02364 videos
manne test-yes 0012595 frames from 00333 videos
manneq test-no 0000728 frames from 00024 videos
man test-maybe 0012831 frames from 00300 videos
mannequin vali 0005928 frames from 00088 videos

Maybe talk in defense about the origins of mannequin challenge at Jacksonville High School and scientists say that ease of access is also probably why it became famous in 2016.

This was actually mannequin challenge version 1 hope they get to extend their dataset just like how real estate was able to be extended.

Visual SLAM and structure-from-motion have no way of determining absolute scale without external information: each of our training sequences is therefore equally valid if we scale the world (including the sparse point sets and the translation part of the camera poses) up or down by any constant factor. This is not an issue when dealing with multiple-image input since the relative pose between the inputs resolves the scale ambiguity, but it poses a challenge for learning any sort of 3D representation from a single input.

4.3 Refining poses with bundle adjustment: 
We next process each sequence at higher resolution, using a standard structure-from-motion pipeline to extract features from each frame, match these features across frames, and perform a global bundle adjustment using the Ceres non-linear least squares optimizer [Agarwal et al. 2016].

Stereo Magnification paper training:
Training details. We implement our system in TensorFlow [Abadi et al. 2016]. We train the network using the ADAM solver [Kingma and Ba 2014] for 600K iterations with learning rate 0.0002, β1 = 0.9, β2 = 0.999, and batch size 1. During training, the images and MPI have a spatial resolution of 1024 × 576, but the model can be applied to arbitrary resolution at test time in a fully-convolutional manner. Training takes about one week on a Tesla P100 GPU.

They didn't even use multiGPU — stereo magnification folks