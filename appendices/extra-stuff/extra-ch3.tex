Maybe talk in defense about the origins of mannequin challenge at Jacksonville High School and scientists say that ease of access is also probably why it became famous in 2016.

Visual SLAM and structure-from-motion have no way of determining absolute scale without external information: each of our training sequences is therefore equally valid if we scale the world (including the sparse point sets and the translation part of the camera poses) up or down by any constant factor. This is not an issue when dealing with multiple-image input since the relative pose between the inputs resolves the scale ambiguity, but it poses a challenge for learning any sort of 3D representation from a single input.

4.3 Refining poses with bundle adjustment: 
We next process each sequence at higher resolution, using a standard structure-from-motion pipeline to extract features from each frame, match these features across frames, and perform a global bundle adjustment using the Ceres non-linear least squares optimizer [Agarwal et al. 2016].

Stereo Magnification paper training:
Training details. We implement our system in TensorFlow [Abadi et al. 2016]. We train the network using the ADAM solver [Kingma and Ba 2014] for 600K iterations with learning rate 0.0002, β1 = 0.9, β2 = 0.999, and batch size 1. During training, the images and MPI have a spatial resolution of 1024 × 576, but the model can be applied to arbitrary resolution at test time in a fully-convolutional manner. Training takes about one week on a Tesla P100 GPU.

They didn't even use multiGPU — stereo magnification authors.