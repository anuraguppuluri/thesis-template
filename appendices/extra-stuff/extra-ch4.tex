Results section:
Here are some evaluations that have been brought up with comparisons with baseline people depth estimation algorithms like MiDAS:


PSNR and SSIM metrics are computed on all pixels during evaluation.

At test time, we use the point
set to compute the scale factor  in the same way as we do
during trainingâ€”

Using the 
hoe they perform against Mannequin Dataset 

Steps to generate SSMI and PSNR
\cite{wang_ssmi_psnr}
\url{https://www.tensorflow.org/api_docs/python/tf/image/psnr}
\url{https://www.tensorflow.org/api_docs/python/tf/image/ssim}

Steps to generate SSMI and PSNR: ---
1) Single view MPI Use model and run inference to generate intermittent views from known angles 
2) Keep aside a frames from 

three datapoints to be got: ---
1) directly do inference from MPI colab and generate PSNR and SSMI for intermittent frames from Mannequin Dataset like actual training procedure?
2) repeat 1) by getting trained weights with only Mannequin Data and using them on colab to infer  
3) repeat 2) by getting trained weights with also RealEstate Data not just Mannequin Data and using them on colab to infer
2) get intermediate disparity maps during trainig 

steps on x axis and loss on y asix
training, validaton and test data plots to go into experiments and results
save loss function values to text file so as to be plotted

About the Kalantari baseline in 2018 mpi model
How were these Kalantri numbers geenrated

Ho do you bring abou tthe baseline 

What is the baseline approach of the authors 

To compare these methods, we measure the accuracy of
synthesized images using the LPIPS perceptual similarity
metric [35] and PSNR and SSIM metrics, on a held-out set
of 300 test sequences, choosing source and target frames
to be 5 or 10 frames apart. At test time, we use the point
set to compute the scale factor Ïƒ in the same way as we do
during trainingâ€”for a fair comparison, we also do this for
the noscale model. Results are in Table 2 (LPIPSall, PSNRall
and SSIMall columns).

Prof Ventura How is this done?


Todo:


Run colmap on test set  [ 10 min ] 
Start from test.py copy to test-ssmi.py - [ 30 min ]
add colemap points path
Add a while(true) and starting with i = 0 outside and i = i + 1 inside the loop 
saving the metrics and log 
Once u get 20 videos done for colemap -- copy to separate small folder and run script in 2) [ 10 min ] 

8/29/2021
Colab intermittent images, draw.io pipelines, paper images
9 pm - 10 pm [ 1 hr ] inference to do non-randomized coding ( model 1 , model 2 ) 
 [ 10 pm - 11 pm ] redo methods + results + conclusion -> intro 
[ 1 hr ] Fix colemap issues for Manequinn only + video resolution issues 
[ 1 hr ] Star re-running Training Manequinn only for 8000+ for training further ( how many iteration do we need to do it --- ) â†’ ( may need to run for 2 full days ) 
Gather colemap results and use one gpu to process on problem videos
Colemap Data gathering for training for model 3) = { realty + mannequin } for 4 full days 
Start training 

(45 mins) 
How is the input image picked and test set picked ?
What happens to the image ? 
What is the output rendered ? 
Between what images is the output metrics calculated ? 

PSNR or Peak Signal-to-Noise Ratio is 

[ 7 PM ]
Metrics : Explain math behind the metrics and how does the library you use compute it with references ? 

â†’ Results : 

Here are the results for the baseline model and the variant retrained on only the video-chat-relevant Mannequin Challenge dataset (current for 20 test videos - all 338 test videos coming soon):
Source-Target Distance PSNR SSIM LPIPS



























 
> Make a table with all the numbers and spaces for the three different models like in original MPI paper 
> generate one set of images : reference, ( with pose in fig description ); target (pose in fig description) and rendered 

The authors of 2020 MPI \ref used the following pointers to qualitatively compare the discrepancies in the results generated by each variant model:
(i) handling of occluded content, 
(ii) unpleasant artifacts at the edges of foreground objects 

In addition to visually checking for these, we find that visually cheking the disparity map is also useful in verifying the quality of the mpi produced   

The authors of 2020 MPI \ref used pointers like qualitatively compare the discrepancies in the results generated by each variant model:
(i) handling of occluded content, 
(ii) unpleasant artifacts at the edges of foreground objects

In addition to visually checking for these, we find that visually cheking the disparity map is also useful in verifying the quality of the mpi produced   

Table for PSNR, SSIM, and LPIPS
More visually appealing graphs coming up

Using the 
hoe they perform against Mannequin Dataset 

Ask professor Ventura about metrics to generate to understand perfrmance 

Steps to generate SSIM and PSNR
\cite{wang_ssmi_psnr}
https://www.tensorflow.org/api_docs/python/tf/image/psnr
https://www.tensorflow.org/api_docs/python/tf/image/ssim

Steps to generate SSMI and PSNR: ---
1) Single view MPI Use model and run inference to generate intermittent views from known angles 
2) Keep aside a frames from 

Does stereo mpi and single view mpi have the same training procedure of suppressing some frames from a video clip as ground truth and predicting these frames and finally minimizing the loss between the rendered frame and the suppressed frame like PSNR and SSMI?



steps on x axis and loss on y asix
training, validaton and test data plots to go into experiments and results
save loss function values to text file so as to be plotted

About the Kalantari baseline in 2018 mpi model
How were these Kalantri numbers geenrated

Ho do you bring abou tthe baseline 

What is the baseline approach of the authors 

To compare these methods, we measure the accuracy of
synthesized images using the LPIPS perceptual similarity
metric [35] and PSNR and SSIM metrics, on a held-out set
of 300 test sequences, choosing source and target frames
to be 5 or 10 frames apart. At test time, we use the point
set to compute the scale factor Ïƒ in the same way as we do
during trainingâ€”for a fair comparison, we also do this for
the noscale model. Results are in Table 2 (LPIPSall, PSNRall
and SSIMall columns).

Prof Ventura How is this done?


1) directly do inference from MPI colab and generate PSNR and SSMI for intermittent frames from Mannequin Dataset like actual training procedure?
2) repeat 1) by getting trained weights with only Mannequin Data and using them on colab to infer  
3) repeat 2) by getting trained weights with also RealEstate Data not just Mannequin Data and using them on colab to infer
2) get intermediate disparity maps during trainig 

Qulaitative analysis can be showing disparity maps 
