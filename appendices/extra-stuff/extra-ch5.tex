% from deep stereo paper the causes for overfitting

% Deep networks have enjoyed huge success in recent
% years, particularly for image understanding tasks [20, 29].
% Despite these successes, relatively little work exists on applying
% deep learning to computer graphics problems and especially
% to generating new views from real imagery. One
% possible reason is the perceived inability of deep networks
% to generate pixels directly, but recent work on denoising
% [35], super-resolution [6], and rendering [21] suggest
% that this is a misconception. Another common objection is
% that deep networks have a huge number of parameters and
% hence are prone to overfitting in the absence of enormous
% quantities of data, but recent work [29] has demonstrated
% state-of-the-art deep networks whose parameters number in
% the low millions, greatly reducing the potential for overfitting.

% from deep stereo on the success of neural nets

% In this work we present a new approach to new view synthesis
% that uses deep networks to regress directly to output
% pixel colors given the posed input images. Our system
% is able to interpolate between views separated by a
% wide baseline and exhibits resilience to traditional failure
% modes, including graceful degradation in the presence of
% scene motion and specularities. We posit this is due to the
% end-to-end nature of the training, and the ability of deep
% networks to learn extremely complex non-linear functions
% of their inputs [25].
% Additionally, although we focus on its application
% to new view problems here, we believe that the
% deep architecture presented can be readily applied to other
% stereo and graphics problems given suitable training data.
% Because
% of the variety of the scenes seen in training our system is robust
% and generalizes to indoor and outdoor imagery, as well
% as to image collections used in prior work.

Maybe if the software ffmpeg or otherwise takes faster and gets better at upsampling maybe that's when I can think of upsampling less-than-720p videos.

How exactly do these things change with the scale of an image? 
Do they correspond exactly or what? Like do the videos correspond exactly to the points produced or what?

This would allow for feeding a lot more images/video-frames to the model, which would further increase the accuracy of the model.

% \url{https://www.pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/}

% Ideal for a headless server?

% possible hypothesis: did cuda support improve disparity map

% possible hypothesis: cuda gpu support is possibly not required for OpenFace inference

% Features of OpenFace like head pose estimation may still work without CUDA recognition by the server either Colab or El Capitan. 

% Need GPU support for maybe MPI training alone and not any other components of the pipeline as the rest of the pipeline is just inference.

% COLMAP will be quicker with GPU as described in \url{https://colmap.github.io/faq.html#available-functionality-without-gpu-cuda}.


% Mainly mpi and maybe even colmap (for inference speed) seem to require GPU/CUDA support. I've been trying to get GPUs to be used by all my packages on Docker like MPI, COLMAP and their dependencies OpenCV, Dlib etc.
% I doesn't seem to work yet. So I'll resort to using these packages on Docker without GPU support for now. 

% In videos we have recording of my insights today - 8/15/21
% been falling behind and didn't report until resukts 

% and update to flagship versions

% the main thing Dr ventura is that the CUDA install was broken and I needed it for multiple programs like dlib, openface, notwithstanding colmap 

% ask prof ventura to update the nvidia drivers 

% why did i go off on a tangent?
% https://stackoverflow.com/questions/43022843/nvidia-nvml-driver-library-version-mismatch
% Available functionality without GPU/CUDA

% https://colmap.github.io/faq.html#available-functionality-without-gpu-cuda
% If you do not have a CUDA-enabled GPU but some other GPU, you can use all COLMAP functionality except the dense reconstruction part. However, you can use external dense reconstruction software as an alternative, as described in the Tutorial. If you have a GPU with low compute power or you want to execute COLMAP on a machine without an attached display and without CUDA support, you can run all steps on the CPU by specifying the appropriate options (e.g., --SiftExtraction.use_gpu=false for the feature extraction step). But note that this might result in a significant slow-down of the reconstruction pipeline. Please, also note that feature extraction on the CPU can consume excessive RAM for large images in the default settings, which might require manually reducing the maximum image size using --SiftExtraction.max_image_size and/or setting --SiftExtraction.first_octave 0 or by manually limiting the number of threads using --SiftExtraction.num_threads.

% nvidia-smi
% Failed to initialize NVML: Driver/library version mismatch

% what is cuda and nvcc all about?
% https://varhowto.com/check-cuda-version/

% dockerfile colmap run needs to be explained with video clip in MAnnequinChallenge

% proof that gpu is being used by colmap in images in MannequinChallenge

% https://linuxize.com/post/linux-time-command/
% time all functions

% make sure I'm able to restart the model at any poit and continue traning whre I left off and add datasets 

% I need info on inference code 

% Ask ventura why did yoyu say the disparity was bad

% Hopefully we are successfully able to use the latest versions of all components of the MPI pipeline for both training and inference sooner than later.  

% Another application would be if we have a VR headset with a camera on it we can track the rotation of the camera and by doing that you're tracking the rotation of the person's head so that you can render the VR content at the right angle

% Ask prof ventura is colmap autmatically redoes all error videos 
% Ask pro ventura about copyright for his own epipolar geometry lectures

% stereo = 2 images pretty close to each other paired in a special way so that you can get really dense estimations of the depth so basically for every pixel you could get a depth estimate rather than some sparse sampling of keypoints
% canonical stereo case only have pure horizontal translation and no rotation and no other translations in Y or Z  

% blueer values are closer and redder values are farther away in disparity maps

% check 7000 train set and 1400 test set of 2018 paper

% why doesn't 2020 and 2018 papers employ SLAM algorithms directly from COLMAP and not indorectly by themsellves or are they refereing to the same slam algorithms

% i.e., essentially combining 2020 paper with this predecessor 
% chaekc the first chaPpter of interduction of 2019 deep stereo for more info abou this
% As a consequence, the network takes much larger strides along the direction of optimization and converges much sooner and with more accuracy than a network using standard gradient descent.

% Actually, the DeepView paper has a beautiful software to customized, visualize, and render any type of MPI! It's kind of like the state of the art MPI manipulator.~\url{https://augmentedperception.github.io/deepview/}. So maybe improve the 2020 MPI html visualizer upto the standarsd of the deep view one or atleast use it to tweak and experiment the various MPI paraeters lik number of layers etc before deploying and training and testing.

% Adam is better than regular stochastic gradient descent but still not superior to Flynn et al.'s~\cite{flynn_deepview_2019} implementation of learned gradient descent.

