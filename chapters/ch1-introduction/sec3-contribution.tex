\section{Contribution}\label{sec:contribution} 

To give a gist of our work, it began by attempting to retrain Tucker and Snavely's state-of-the-art end-to-end fully-convolutional single-view view synthesis with MPIs CNN~\cite{single_view_mpi} on the MannequinChallenge dataset. We hypothesized --- as was also hinted at in the paper --- that such retraining would be sufficient to generate high quality MPIs of scenes involving close-up shots of people, typical of video chat settings. The original model is able to do the same for real estate scenes. We then went on to compare the inference results of this primary model variant with those of another variant trained on the MannequinChallenge dataset extended by the RealEstate10K dataset, taking the pretrained Tucker and Snavely model as baseline. This was so we could determine the best variant to apply to the domain of 3D video chat. Such application was conceived to be by way of a two-way rendering of appropriate novel views of concurrent dynamic scenes viewed by one-on-one video chat participants in both directions simultaneously. In the two-way pipeline, a novel view of a video frame would be rendered every time a change in head pose is detected in the participant in the opposite frame. To our knowledge, MPIs have not been used in 3D video chat so far. We publish the code used to fill in the missing parts of Tucker and Snavely's publicly available training and testing pipelines along with instructions for curating and taking advantage of both datasets for view synthesis in video chats.
