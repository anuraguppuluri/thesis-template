\section{Learning MPIs}\label{sec:learning-mpis} 

Some of the major challenges in high-quality novel view synthesis include synthesizing pixels occluded in one or more of the provided views, disentangling and localizing ambiguous pixels at/near the boundaries of foreground and background objects, localizing pixels at transparent, translucent, reflective, or texture-less surfaces, etc. Moreover, whereas interpolating novel views at desired viewpoints lying within the convex hull of given viewpoints is easier to achieve than extrapolating significantly beyond the baselines (distances between camera centers) of input views, these challenges can emerge in either case. So far, it has been found that learning view synthesis is the way to go for tackling them all in one shot.

Before the Machine Learning (ML) boom in Computer Vision (CV) circles in 2012, convolutional filters had to be handcrafted and dexterously layered one atop the other before input views could be subjected to them and various types of features could be extracted in the process of rendering novel views. All the aforementioned view synthesis challenges had to be manually targeted by way of devising various combinations of these filters. This meant a high proportion of artifacts induced in novel views could be left unresolved. Since the time that the efficacy of CNNs in CV was proven by Krizhevsky, Sutskever, and Hinton~\cite{krizhevsky_imagenet_2012} in 2012, to the delight of the CV community, the need to handcraft filters was obviated by ML models that learned to design all required convolutional filters on their own in their various hidden layers. These self-taught filters are defined by the weights and biases in each hidden layer neuron. The weights and biases constantly improve during training and the convolutional filters defined by them are specific to the datasets they are trained on, with some degree of generalizability to other datasets. If trained well under effective hyperparamter tuning, learned filters can evolve to surpass manual filters in addressing occlusion, transparency, reflection, and other image synthesis challenges.

View synthesis can lend itself to being a semi-well-posed to well-posed learning problem where two or more images of a scene can be shot and an ML model can be exposed to one of more of these images while being expected to predict one or more of the remaining views that have been withheld from it. The quantitative difference between the corresponding predicted and withheld (as ground truth) views will then be the loss that the ML training seeks to minimize. Since end-to-end view synthesis without an intermediate representation is still largely unrealized, the popular way to synthesize novel views is to learn an intermediate representation of the scene common to the input views and use this intermediate representation to render novel views. The MPI intermediate representation has proven to be one the most effective representations for this purpose with implications as significant as real-time high-quality spatially-consistent view synthesis.

\subsection{Seminal Work}\label{subsec:seminal-work}

The roots of the MPI representation may be traced back to seminal papers such as 1996's Collins~\cite{collins_space-sweep_1996}, 1998's Shade et al.~\cite{shade_layered_1998}, and 1999's Szeliski and Golland~\cite{szeliski_stereo_1999}. Collins perfected the concept of Plane Sweep Volumes (PSVs), Shade et al. introduced layered depth images, and Szeliski and Golland introduced the actual MPI representation itself. These groundbreaking techniques have also been compared in Scharstein and Szeliski~\cite{scharstein_taxonomy_2002}. 

\begin{figure}[!h]
    \includegraphics[width=1\columnwidth]{figures/plane-sweep-volume.png}
    \caption{Plane Sweep Volume Representation~\cite{svetlana_2019}}
    \label{fig:plane-sweep-volume}
\end{figure}

Collins~\cite{collins_space-sweep_1996} applied the PSV representation to the problem of reconstructing the 3D scene from multiple views while simultaneously performing feature matching across all views sharing common features. Feature matching is the process of matching corresponding ``features of interest" characterized by their repeatability across multiple views of the same world scene. Examples include keypoints, corners, edges, objects, etc. Matched views can be rectified and used for triangulating depth, etc., as mentioned in section~\ref{sec:motivation}. In the author's implementation, instead of going for a resource-intensive 3D representation that would require splitting the entire 3D scene space into voxels and reprojecting\footnote{projecting to a target plane by unapplying and applying the homographies needed to project to the source and target planes, respectively, while accounting for surface normals, plane offsets, camera rotations and translations, etc., as described in subsection~\ref{subsec:base-papers}} all feature points from all views in such manner that the reprojected light rays passed through this uniformly partitioned space, he sampled the 3D scene space at various 2D planes along the depth ($z$) axis as if capturing just one 2D plane sweeping though it, at various instants in time. He partitioned the sweeping plane into cells and allowed each reprojected light ray to vote for a group of cells that fell within a certain radius of the point of intersection of the light ray with the plane. This accounts for the fact that rays from corresponding feature points across all views may not converge most of the time due to reprojection errors. He then chose the z-coordinate of the sampled plane containing the cell with the maximum votes for a feature point to be the z-coordinate of the feature point in the world scene. The $x$ and $y$ world coordinates would be defined by this winning cell. The victor cell would also determine the 2D feature point correspondences simultaneously just by virtue of the converging rays being retraced to their respective originating views. PSVs, in their various reimplemented forms, have become almost synonymous of layered volumetric representations these days (Figure~\ref{fig:plane-sweep-volume}).

Shade et al.'s~\cite{shade_layered_1998} Layered Depth Image (LDI) scene representation is similar to MPI scene representation in that both MPI and LDI consist of a series of fronto-parallel planes facing a chosen reference viewpoint and placed at varying depths from it. These planes contain the RGB information of the original pixels of the scene's image(s), segregated according to depth. MPI differs from LDI (and PSV) in that it has alpha masking effects at each layer, as it is generated with alpha transparency maps for each layer. Also, MPI has fixed depths for each layer as opposed to the variable layer depths of LDI (and PSV). But in both cases, by virtue of layering, users are able to experience a simulation of what happens when they move their heads while looking at a scene in the world --- they are able to look around foreground objects that occlude background ones. 

Szeliski and Golland~\cite{szeliski_stereo_1999} first introduced the MPI representation for purposes of stereo matching with simultaneous RGBA estimation at each matched pixel. Stereo matching, otherwise called disparity mapping, uses feature matching techniques like SIFT\footnote{Scale-Invariant Feature Transform~\cite{lowe_distinctive_2004}} in pixel-and-sub-pixel-wise disparity estimation for 3D scene reconstruction from rectified stereo images. The authors' framework was the first to extract high-accuracy depth, color, and transparency maps for several images at a time, operating even at sub-pixel levels. They were able to enforce sub-pixel accuracy and perform effective matte separation of foreground and background elements despite the usual 3D vision challenges such as occlusions, etc., because they came as close to modern ML reimplementations as possible. They implemented various loss functions such as a pixel-wise weighted photometric L1 norm between the input and reprojected images, a per-pixel smoothness constraint on the RGBA values allowed in the reprojected images, etc. They then performed an iterative refinement of the estimated RGBA values with the help of a gradient\footnote{vector of partial derivatives of the function(s) to be optimized} descent algorithm designed to optimize a combination of all these losses, but sans the explosive power of neural networks.

\subsection{Influential Work}\label{subsec:influential-work}

Flynn et al.~\cite{deep_stereo_2016} was the first to apply CNNs in an end-to-end manner to novel view synthesis from diverse collections of indoor and outdoor imagery in the wild, given the availability of camera parameters\footnote{camera intrinscics such as focal length and principal point and camera extrinsics/pose such as position and orientation} for each input image. Their paper describes why it would be unwise to expect a typical present-day CNN to synthesize any ground-truth target image without being provided with the pose of the view as well --- the network would needlessly be learning epipolar geometry itself! Epipolar geometry --- the geometry of binocular and multi-view stereo vision --- gives us the epipolar constraint $x'^T F x = 0$ between all corresponding points $x$ and $x'$ on a stereo pair. Here, $F$ is called the fundamental matrix and is derived from the intrinsic and extrinsic parameters of the stereo cameras involved. To circumvent such an indeterminable and expensive pixel-to-pixel training scenario, the authors had PSVs (Figure~\ref{fig:plane-sweep-volume}) come to the rescue. They supplied all input views required to synthesize a target view as separate PSVs to their network. Each input plane sweep would contain all pixels of the respective input view reprojected onto a chosen number of planes at chosen depths in the usual ``stack of acetates" manner, with the panes all having their viewpoints match with the target view's. The plane that each RGB pixel gets reprojected onto will also determine the availability of the pixel (as alpha values ranging from 0 to 1) to the surrounding voxels of the PSV. The plane sweep of each input view has the pose information of the view implicitly encoded in it just by virtue of its construction. Moreover, the plane sweeps of all input views of the same scene trivially enforce the epipolar constraint as matching pixels across these originating input views may be located in the same depth-wise column of each plane sweep. Each of these depth-wise columns may then be computed upon by the network independently of other columns, in producing the corresponding synthesized target pixel. The network learns to predict the best weight and color for each reprojected pixel on all input planes, so it may perform a weighted summation of these estimated pixel colors and obtain a final predicted target pixel color. Such averaging has a smoothing effect over the color values of the synthesized target image. The error signal that is iteratively minimized by the training is given by the pixel-wise $L_1$ (absolute difference) loss between the actual target color $C_{i,j}^t$ and the synthesized target color $C_{i,j}^s$ at each pixel $(i,j)$:

\[ L = \sum_{i,j} |C_{i,j}^t - C_{i,j}^s| \]

Kalantari et al.'s~\cite{kalantari_2016} model learns to interpolate novel views in the 8x8 central view grid of a Lytro camera containing a microlens array. It was the state-of-the-art learning-based view synthesis model prior to Zhou et al.'s~\cite{zhou2018stereo} \textit{stereo magnification} MPI model. It is composed of disparity and color predictor components in the form of simple 4-layered sequential CNNs. The training signal it optimizes is given by the $L_2$ (squared difference) pixel reconstruction loss between each pair of original and interpolated target views.

Both Flynn et al. and Kalantari et al. are unable to train on training images in their entirety. Instead, they extract patches of training images for their models to train on. This is because, unlike how Zhou et al.'s model is designed to predict a global scene representation once for a pair of views sharing the same scene and render many novel views with it at near-real-time speeds, the former models are designed to predict each novel view in an end-to-end fashion independently of other novel views and so have to rerun their prediction pipelines every time, making novel view synthesis prohibitively slow for high-resolution and real-time applications. Moreover, when rendering nearby views, the former methods produce much more artifact-ridden, spatially incoherent views compared to the views inferred by Zhou et al. What Zhou et al. has going for it in these scenarios is an implicit smoothness prior imposed by the common scene representation over the color and depth values being inferred for each synthesized nearby view. 

What also comes close to the MPI representation is the layered representation of Penner and Zhang~\cite{penner_soft_2017}. But then again, in all these prior methods, a unique scene representation is predicted in the reference coordinate frame of each target view to be rerendered, negatively impacting view synthesis efficiency. Other innovative MPI-related papers released subsequently to Zhou et al. and leading up to Tucker and Snavely's~\cite{single_view_mpi} \textit{single-view} MPIs are 2019's Srinivasan et al.~\cite{srinivasan_pushing_2019}, Mildenhall et al.~\cite{mildenhall_local_2019}, and Flynn et al.~\cite{flynn_deepview_2019}. Srinivasan et al. improved the quality and increased the spatial and disparity ranges of predicted MPIs and rendered views, by bringing in a 3D CNN architecture, training on random-resolution views, and introducing an optical flow constraint over the appearance of occluded content in rendered views. Mildenhall et al.'s model converts an grid of irregularly sampled views into MPIs i.e., mini-light-fields, and blends nearby local light fields to render novel views. They were able to establish a minimum density of sampled views required for robust rendering, which turned out to be 4000$\times$ less than the Nyquist frequency required to prevent aliasing. Flynn et al.~\cite{flynn_deepview_2019} replaced the update step\footnote{involving step size and other parameters like priors/biases} of the network's gradient descent algorithm with a CNN that learns the various gradient descent parameters instead. As a consequence, the network takes much larger strides along the direction of optimization and converges much sooner and with more accuracy than a network using standard gradient descent. However, these methods do not tackle the single-view approach for generating MPIs.

\subsection{Base Papers}\label{subsec:base-papers}

Zhou et al.~\cite{zhou2018stereo} was the first to implement view extrapolation to significantly larger baselines (up to 8$\times$ input baselines) than prior work --- a process they call stereo magnification. They use a stereo pair of images to learn an MPI prediction network in the following manner:

\begin{itemize}
    \item The camera parameters $c_1\ =\ (p_1,\ k_1)$ and $c_1\ =\ (p_2,\ k_2)$ of the stereo pair, $(I_1,\ I_2)$, are also needed for the prediction process, along with the target image $I_t$ and its parameters $c_t$. Here, $p_i$ and $k_i$ are the camera extrinsics and intrinsics of the respective images.
    \item The viewpoint of one image of the stereo pair, $I_1$, is used as the reference viewpoint for the MPI to be predicted at. Hence, $p_1$ would be the identity pose $[I|\boldsymbol{\mathit{0}}]$.
    \item The goal is to learn a network that generates an MPI representation (Figure~\ref{}) with inputs $(I_1,\ I_2,\ c_1,\ c_2)$ such that when the MPI is rendered at viewpoint $c_t$, it would produce the target image $I_t$. 
    \item As demonstrated by Flynn et al.~\cite{deep_stereo_2016}, an effective way to encode pose information for training is via a PSV. Hence, the input to their customized encoder-decoder type network includes a PSV version of $I_2$ $(\hat{I_2})$ with the planes reprojected into the final MPI's viewpoint, $c_1$, and with the entire plane sweep concatenated internally and with an unaltered $I_1$ along the three color channels. The depth planes of $\hat{I_2}$ are also chosen to coincide with the ones of the output MPI.
    \item The 3D structure of the scene depicted by $I_1$ and $I_2$ is automatically learnt by the network by merely being able to compare $I_1$ with each of the reprojected images of $I_2$ in the input stack $(\hat{I_2^1},\ \hat{I_2^2},\ \ldots,\ \hat{I_2^D},\  I_1)$, where $D$, again, refers to the total number of MPI depth planes (Section~\ref{sec:motivation}). The depth at each pixel of any known or novel view of the scene must be the depth of the plane where $I_1$ and $\hat{I_2}$ concur.
    \item In order to reduce resource consumption due to network over-parameterization, the network's initial outputs do not consist of separate RGBA maps for each MPI layer but rather just a ``background" image intended to capture pixels occluded in $I_1$ and a set of color blending weight maps and alpha maps for each MPI layer.
    \item The actual RGB values in each layer, $C_d$, are then easily computed by taking the per-pixel weighted average of $I_1$ and the predicted background image $\hat{I_b}$: \[ C_d = w_d \odot I_1 + (1 - w_d) \odot \hat{I_b} \] Here, $\odot$ is the Hadamard product and $w_d$ refers to the RGB blending weights from the initial network output, specific to MPI layer $d$.
\end{itemize}

\begin{figure}[!h]
    \includegraphics[width=0.75\columnwidth]{figures/inferred-mpi.png}
    \caption{Inferred MPI~\cite{zhou2018stereo}}
    \label{fig:inferred-mpi}
\end{figure}

The rest of the training pipeline consists of the rendering of the MPI at the target viewpoint, $c_t$, and the gradient descent algorithm involving a VGG perceptual (similar to LPIPS~\cite{zhang_unreasonable_2018}) loss function between the rendered view and ground-truth target view. The perceptual loss is proven to be more robust than unmodified pixel reconstruction losses like $L_1$ and $L_2$ norms. Adam gradient descent algorithm is used (similarly to Tucker and Snavely~\cite{single_view_mpi}) to optimize this loss. Adam~\cite{kingma_adam_2017} is better than regular stochastic gradient descent but is still not superior to Flynn et al.'s~\cite{flynn_deepview_2019} implementation of learned gradient descent. Rendering an MPI first involves warping each RGBA MPI layer onto the target camera's image plane using the standard inverse homography or reprojection operation~\cite{hartley_multiple_2004}, as illustrated in figure~\ref{fig:standard-inverse-homography}. But, anticipating usual reprojection mismatches, they resample each pixel to be warped by bilinear interpolation with respective four-grid neighbors. These rerendered MPI layers are then alpha-composited in back-to-front order to get the final predicted target view.

\begin{figure}[!h]
    \includegraphics[width=0.75\columnwidth]{figures/standard-inverse-homography.png}
    \caption{Standard Inverse Homography or Reprojection~\cite{brann_forelesninger_2016}}
    \label{fig:standard-inverse-homography}
    {\small Here, the 3D point $\boldsymbol{X}$ on the MPI plane in the world is the \textit{homogeneous} version (determined up to scale) of its projection $\boldsymbol{x}$ on the reference camera's image plane in camera coordinates, i.e., with the camera's image plane centered at the camera center, $c_{ref}$. More precisely, $\boldsymbol{X} = [X,Y,d_m]^T \sim \tilde{\boldsymbol{x}} = [X/d_m,Y/d_m,1]$. This is because all MPI world planes are fronto-parallel to the reference camera and their equations can be given by $\boldsymbol{n_m \cdot \tilde{x}} + a = 0$, where $\boldsymbol{n_m} = [0,0,1]$ is the plane normal and $a = -d_m$ is the plane offset from $c_{ref}$. The projection $\boldsymbol{u}$ on the reference camera's image plane in regular image coordinates is attained by applying reference camera intrinsics $\boldsymbol{K_{ref}}$ to $\boldsymbol{x}$. Since the MPI is not necessarily fronto-parallel to the target camera $c_k$, $\tilde{\boldsymbol{x'}}$ need not be $[X/d_m,Y/d_m,1]$ even though $\boldsymbol{X} \sim \tilde{\boldsymbol{x'}}$ as well. $\boldsymbol{u'}$ and $\boldsymbol{K_k}$ similarly belong to the target camera, as does target camera pose (relative to reference camera) $[R_k|\boldsymbol{t_k}]$. The world plane \textit{induces} the homography $H = \boldsymbol{K_k} (R_k - \boldsymbol{t_k} \boldsymbol{n_m}^T / a) \boldsymbol{K_{ref}}^{-1}$ between the image planes of $c_{ref}$ and $c_k$, so we can go from $\boldsymbol{u}$ to $\boldsymbol{u'}$. To go from $\boldsymbol{u'}$ back to $\boldsymbol{u}$, we'd use $H^{-1}$~\cite{zikuicai_derivation_2019}.}  
\end{figure}

Zhou et al.'s methods are ingenious in a number of ways. Some examples include them training to predict novel views at varying distances from input views so as not to overfit to predicting only up to a limited number of baselines, their use of assorted but apt convolutional layers such as dilated convolutions to bring back larger scene contexts at lower computational costs and fractionally-strided convolutions~\cite{prove_introduction_2018} with skip connections~\cite{adaloglou_intuitive_2020} from preceding layers to capture even the finer texture details, and their use of VGG perceptual loss to retain these intricate micro textures besides macro object geometries in synthesized views. Also commendable is their meticulous RealEstate10K dataset creation process which was continued by Tucker and Snavely in bringing the dataset to it's current state~\cite{zhou2018stereo}. Knowing that state-of-the-art Structure from Motion (SfM) and bundle adjustment\footnote{initial scene reconstruction, camera calibration (including field of view estimation), and pose estimation for a candidate pair of scene views, followed by simultaneous iterative refinement of the 3D scene structure and hitherto estimated camera parameters, using each additional view of the scene for feature matching as well} algorithms like COLMAP~\cite{} are not yet optimized for camera tracking in videos, they first subject candidate real estate YouTube videos to Simultaneous Localization And Mapping (SLAM) techniques such as ORB-SLAM2~\cite{} to obtain initial camera parameter estimates for all consecutive frames tracked. Consecutive, here, implies that each tracked frame's viewpoint is no farther than a certain percentage of the average of its two neighboring viewpoints. This process naturally breaks a video apart into clips with smoother camera motion. They then process all video clips obtained this way with COLMAP to obtain a sparse 3D point cloud reconstruction of the scene in each clip and a refined set of camera parameter estimates for all frames. As a final step, they ``scale-normalize" each subsequence and its reconstructed camera parameters and 3D points in one shot by scaling just the point cloud up or down so the nearest set of points is at a fixed distance from the cameras. SfM involves the estimation of the (generally sparse) 3D structure of a static scene from the multiple (usually unstructured) views of a (often uncalibrated) camera moving around the scene, accompanied by the simultaneous estimation of respective camera parameters. It is essentially a more generic version of Multi-View Stereo (MVS), which itself is an extension of stereo matching and requires known camera parameters to reconstruct (mostly) dense 3D points clouds. COLMAP is capable of both SfM and MVS. Both SfM and MVS can utilize bundle adjustment similarly to SLAM from the robotics community. SLAM doesn't stop at bundle adjustment but rather proceeds to map out the entire terrain encountered by a robot by making connections between camera trajectories, viewed scenes, etc. 

Some major observations made by Zhou et al. in their ablation studies include 
maybe mention the benefits of color blending wirghts as opposd to the other ablation studis of no FG + BG + blending weights?

larger output space and less utiization of refence image makes learning 3D scene structue harder? or just the RGB structue?

performance of model improves as more number of depth planes are used

greter the offset netween the refernce and renederd views the more planes are needed to render accurateky

2018 paper successfully applied it to soners qute difernt frm realeste 10K 

but can we say the same about 2020 paper? time to find out!

extrapolating beyond the limits of the reresentaion gives a " stack of cards " effect 


Tucker and Snavely~\cite{single_view_mpi} was the first to implement single-view view synthesis on videos in the wild.
its fascinating to see how they were able to achive the coveted efficient single view view sytheis coeted by the vision and graphcs community 

A special issue occurs when overseeing such a system via view synthesis, due to the uncertainty in the input data's global scale. The authors address this issue with a method of scale invariant view synthesis that makes use of sparse point sets generated during the training data generation process. The authors present an edge-aware smoothness loss that prevents depth maps created from predicted MPIs from becoming excessively fuzzy, even in the absence of depth supervision.

While the authors' purpose is view synthesis rather than depth prediction, they can easily generate disparity maps from the MPIs and use these to assess depth performance. The authors demonstrate the ability to predict MPIs for view synthesis from single image inputs without the need for ground truth 3D or depth information, and they introduce a scale-invariant approach to view synthesis that enables them to train on data with scale ambiguity, such as that derived from online video. A possible next path would be to combine MPI prediction with adversarial losses to determine whether it is possible to produce better, and more realistic, inpainting.


In the abstract they say they use scale-invariant view synthesis as supervision by which they mean they use additional views of the scene  as supervision (possibly unscaled?)


Single-view view synthesis only during inference. During training 2020 model learns to predict an MPI in one stage without requiring any post-processing steps but nevertheless requires multiple images of the same scene as supervision in the one-stage training pipeline.

Single view view synthesis is an diffenert fromo the data collection aspedct of the 2018 paper becasue they retain the sparse point clouds and a record of whicvh points were tracked in each frame (refereed to as vidsible points)


how do they even come up with depth pixels for both LDI/MPI? do they already know depth from ground truth ????
and how do they calculate depth ???

we can give the number of layers to the network - does it make a difference? 
when you have more layers you get more depth info - more clarity in the rendered image -


somehow quantizing the depths 
tyhe 3d objecxt wiil fall in different plab=nes

why did they include colmap point could vs 

mention about the net transmittance of the color that is beautifuly explained in the the over composition in the deep view paper

scale-invariant view sythesis for supervision
PSNR AND SSIM metrics are compued on all pixels during evaluation

2020 doesn't require ground-truth 3D or depth
predicted BG image to inpaint what lies behind foreground objects

premultiplied alpha color images needs to mentioned from deev view page 2369

\begin{figure}[!h]
    \includegraphics[width=1\columnwidth]{figures/}
    \caption{Fronto-parallel planes in MPI layered representation}
    \label{fig:mpi-layered-representation}
\end{figure}
Fig 3. Example of fronto-parallel planes

However, in fact, the coordinates of picture dots cannot be determined with arbitrarily high precision. Rather than that, other sources of noise, such as geometric noise caused by lens distortion or interest point recognition mistake, cause inconsistencies in the picture coordinates that are measured. As a result, the lines created by the associated picture points do not always meet in three-dimensional space. The objective is then to identify a three-dimensional point that fits the observed picture points optimally. Numerous approaches exist in the literature for how to define optimality and how to determine the optimal 3D location. Due to the fact that the various approaches are based on distinct optimality criteria, they generate significantly varied estimations of the 3D point x when noise is present.

Depth maps are derived from the predicted MPIs and not predicted in parallel with the MPIs

2020 paper uses the same data curation process as the 2018 paper