\chapter{Methods}\label{ch3:methods}

The objective of this work has been to freely rerender concurrent one-on-one video chat frames from the points of view of both participants bidirectionally and in real-time. This would help simulate the experience of conversing face-to-face with a person in the real world. We adopted Tucker and Snavely's~\cite{single_view_mpi} single-view MPI network, for it is the first state-of-the-art open-source single-view view synthesis network, and it has been quite popular among various enterprises and organizations since its release in 2020. When we initially ran the publicly available inference part of the network on a video chat frame, we found that the generated disparity map (Equation~\ref{eq:disparity-map}) was visually inaccurate. Comparatively (Figure~\ref{fig:great-off-kilter-disparity}), the inferred disparity map would be much more visually accurate whenever a real estate video frame would be processed. The latter outcome is to be expected because Tucker and Snavely's model was originally trained on RealEstate10K~\cite{zhou2018stereo} video dataset. Specifically, certain aspects of the synthesized views, such as image sharpness, would be pretty compelling for the real estate category of video frames by virtue of the model having been efficiently tweaked and extensively trained by the authors (given contemporary hardware limitations). Yet, synthesized video-chat-related frames alone would seem unnaturally concave/convex at arbitrary positions within each rerendered frame, not to mention the loss of perspectivity and the induction of random distortions occurring within the frame as well.

\begin{figure}[!h]
    \includegraphics[width=1\columnwidth]{figures/great-off-kilter-disparity.png}
    \caption{Disparity Heat Maps Synthesized by Tucker and Snavely's Model~\cite{single_view_mpi} for Real Estate and Video Chat Frames}
    \label{fig:great-off-kilter-disparity}
    {\small The disparity map on the left encodes a real estate scene, and the one on the right encodes a video chat scene. The real estate map successfully shows appropriate heat/depth gradations from the hottest/closest armrest region on the bottom right to the coldest window regions toward the back of the scene. The video chat map, on the other hand, counterintuitively shows that the face of the girl in the scene is situated behind the body, and the couch in it is somehow disjointed.}  
\end{figure}

\section{Approach}\label{sec:approach} 

As a primary step (Figure~\ref{fig:mpi-training-pipeline}), we attempted to increase Tucker and Snavely's depth prediction accuracy for video-chat-relevant frames containing close-up shots of people so that we may see a drastic reduction in the number of artifacts induced in synthesized frames. This involved curating and utilizing both RealEstate10K and MannequinChallenge~\cite{li2019learning} datasets. The latter contains video frames that resemble video chat scenes: it is composed solely of scenes of people pretending to be mannequins while a camera moves around them, flowing seamlessly from scene to scene. Essentially, we performed transfer learning~\cite{radhakrishnan_what_2019} with the pretrained weights of Tucker and Snavely's model, by \textit{fine-tuning}/\textit{refitting} them to a dataset other than the one they were originally trained on. Secondly (Figure~\ref{fig:3d-video-chat-rendering-pipeline}), we introduced the head pose detection submodule of OpenFace 2.2~\cite{baltrusaitis_openface_2018} into the inference pipeline of Tucker and Snavely, so that ``\textit{viewee}" video frames may be rerendered at the head pose obtained from ``\textit{viewer}" frames. We considered a few state-of-the-art open-source head pose estimation models, including WHENet~\cite{zhou_whenet_2020} --- for its speed and consistency. We ultimately chose OpenFace 2.2 because it works well with the Deep Learning (DL) framework used by Tucker and Snavely (TensorFlow 2.2) and can be installed in the same dockerized environment as COLMAP~\cite{schoenberger2016sfm,schoenberger2016mvs} and the rest of the dependencies needed by our comprehensive pipeline. 

\begin{figure}[!h]
    \includegraphics[width=1\columnwidth]{figures/mpi-training-pipeline.png}
    \caption{MPI Training Pipeline}
    \label{fig:mpi-training-pipeline}
\end{figure}

Out of the non-exhaustive set of network components made publicly available by Tucker and Snavely~\cite{single_view_mpi}, a comprehensive inference pipeline on Google Colaboratory (Section~\ref{sec:code-sources}) was one. It immensely helped us with our OpenFace integration and gave us the ability to visualize and present our results and demos in chapter~\ref{ch4:experiments-results} and everywhere else. They couldn't reveal certain other aspects of their codebase due to their proprietary natures. This prompted us to go about recreating Tucker and Snavely's DispNet-like model~\cite{mayer_large_2016} first before retraining it on requisite datasets and repurposing it for video chat view synthesis. We recreated parts of the model from the code released (Section~\ref{sec:code-sources}) by the authors involving their network definition (convolutional layers, kernel sizes, etc.) and the code used by them for rendering views from new camera positions with homographies and related operations (Equation~\ref{eq:rerendered-target}). We then put together other aspects of the network that called for a more involved recreation process, like the data loader part and the loss functions (Equation~\ref{eq:aggregate-loss}). Requisite components of input data, including point clouds, had to be extracted and loaded in. One of the key features of Tucker and Snavely is to use sparse point cloud data to make the view synthesis loss scale-invariant (Subsection~\ref{subsec:base-papers}). To obtain such inputs, we processed both datasets with COLMAP and wrote a custom data loader. We took inspiration from Zhou et al.'s~\cite{zhou2018stereo} stereo MPI paper for building the data loader, for the code they tailored to load in data (Section~\ref{sec:code-sources}) was refactored and reused by Tucker and Snavely as well. Their implementations of subsequence selection and random cropping proved pretty useful.

\begin{figure}[!h]
    \includegraphics[width=0.60\columnwidth]{figures/3d-video-chat-rendering-pipeline.png}
    \caption{3D Video Chat Rendering Pipeline}
    \label{fig:3d-video-chat-rendering-pipeline}
\end{figure}

We retrained the recreated network in two different ways. One group of model variants was fine-tuned exclusively on the video-chat-relevant MannequinChallenge video dataset~\cite{li2019learning}, which is $\sim$96\% smaller than RealEstate10K in training data as of this writing. The other set of variants was retrained on a combination of both datasets by having the model pick same-sized batches of training data (Subsection~\ref{subsec:base-papers}) randomly and alternatingly from both datasets. We considered addressing this inherent data imbalance problem by making the model pick an appropriate proportion of RealEstate10K frames for every MannequinChallenge frame randomly selected. However, we ultimately voted against it in favor of resolving more pressing issues such as the training errors mentioned in section~\ref{sec:implementation}. We are grateful to the authors of Tucker and Snavely for forewarning us that there is a risk of overfitting to the much smaller MannequinChallenge dataset, even though it was generally mentioned in both Zhou et al. and Tucker and Snavely that the stereo and single-view models were quite generalizable to domains besides real estate footage. Hence, we felt the need to deploy the second set of variants to help access this risk. We could also have taken another transfer learning route of freezing all but the last few layers of the model to possibly reduce overfitting, but we chose to unfreeze all layers in favor of making the variants wholly robust. The layers were thus free to learn and evolve based on the MannequinChallenge data they were newly exposed to. We stack these variants up against each other and also against the pretrained single-view model and compare their performances in chapter~\ref{ch4:experiments-results}. Finally, after introducing the head pose estimation API of OpenFace 2.2 into the inference pipeline of the variants, we converted estimated head orientations into a form amenable to rendering with MPIs. This involved manipulating yaw, pitch, and roll head angles, and the MPI helper functions provided by Tucker and Snavely went a long way in making this possible as well. We also visually verified for if the rerendered frames were getting seemingly aligned with the extracted head poses or not.

% \input{chapters/ch3-methods/sec2-data}
% \input{chapters/ch3-methods/sec3-implementation}

