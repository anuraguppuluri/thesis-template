\chapter{Methods}\label{ch2:methods}

\section{Approach}\label{sec1:approach} 

\section{Hypotheses}

 hypotheses of this thesis are as follows:
1) Te MPI 2020 paper, which produces good disparity maps for <the opposite of close-up shots> <long shots of objects/landscapes> \cite{}, is unable to do so for close-up shot of people/objects. When we re-recreated the 202 Hence, our first hypothesis is that re-retraining the re-created 2020 MPI model on  

we appreciate the authors for precaution ing us in extending our dataset with the realestaTE 10K DATRAUSET


xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Stereo Magnification (2018) paper introduces the MPI representation and also explains how the data was processed. Some parts of the code are refactored and reused in the 2020 MPI paper like how the 2018 paper loads data in: in particular the data loader (loader.py, datasets.py) does subsequence selection and some slight random cropping.

richard tucker email very very important:

In terms of relation to the previous MPI papers: Stereo Magnification (2018) introduces the representation and also explains how the data was processed. The code itself is separate but you could look at how it loads data in: in particular the data loader (loader.py, datasets.py) does subsequence selection and some slight random cropping.

From the code we have released (https://github.com/google-research/google-research/tree/master/single_view_mpi) you can see:
  • our the network definition (convolutional layers, kernel sizes, etc) (nets.mpi_from_image)
  • how to render views from new camera positions (mpi.render), including all the homography stuff

What you don't have is (a) the implementation of the losses, and (b) data including point clouds and a way to load it in.
One of the key points of Single-View View Synthesis was to use sparse point cloud data to make the view synthesis loss scale-invariant. To obtain such data you would have to process the mannequin dataset to obtain point cloud and/or depth, for example with COLMAP (maybe you have already done that), and then you could write a data loader or extend the one from Stereo Magnification.

Is your Mannequin Dataset this one: https://google.github.io/mannequinchallenge/www/index.html? If so, a note of caution: it's quite a bit smaller than RealEstate10K, and so there is a risk of overfitting. (I have tried training on something like it myself, and I found that it was better to use a combination of both RealEstate and MannequinChallenge than just MannequinChallenge alone). But anyway, that's something to worry about after you are able to train.

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Whereas the 2018 model claioms to gebreraLIZER TO OTHER DATASEYT WIERHOUT RETREAINGU, no suvk ckaimsd gave been madfe by the 2020 model

When we ran the inference part of the 2020 Single-View MPI model \footnote{Deep Learning terms such as model, network, and CNN have been used interchangeably throughout this thesis.} on a video chat frame and found that the generated disparity map (a by-product of MPI inference in this network architecture) was visually inaccurate, it became the starting off point for our thesis. Comparatively, the inferred disparity map would be much more visually accurate when a real estate video frame was processed. The latter outcome is to be expected because the 2020 Single-View MPI model was exclusively trained on the RealEstate10K video dataset.   

\subsection{Problem Statement 1}\label{subsec1:problem_statement_1}

\textbf{How to increase the 2020 Single-View MPI model’s depth prediction accuracy for video chat frames or any other video-chat-relevant image with close up shots of people.}

As inference was one of the only parts of the network made publicly available by the authors on GitHub due to the proprietary nature of some other aspects of their code, the first step in addressing problem statement 1 was to recreate the 2020 Single-View MPI training procedure.

Recreating and retraining the network involved the following:
\begin{itemize}
    \item Taking the readily available network information from the paper --- like details about the various types of fully-convolutional encoder-decoder layers involved, etc. --- and putting it in place with other aspects of the network that called for a more involved recreation process like the data loader part and the loss function.
    \item Processing all the videos from the required datasets with an ORB-SLAM2 package called COLMAP to extract the 3D point clouds of the scenes in each video frame which makes up one-half of the input to the training pipeline, the other half being the actual video frames themselves.
    \item Attempting to train the network on only the video-chat-relevant Mannequin Challenge video dataset \cite{li2019learning} which is $\sim$90\% smaller than the RealEstate10K dataset.
    \item Fixing the programming errors encountered during training.
    \item Retraining the recreated model on not just the Mannequin Challenge dataset but also the RealEstate10K dataset by clubbing both datasets together to be used as a singular source of input to the recreated 2020 MPI model.
\end{itemize}

The main reason for extending the Mannequin Challenge dataset with the originally used RealEstate10K dataset was so Hypothesis B of this thesis could be proved: ---

\paragraph{Hypothesis A:}
The recreated model trained only on the video-chat-relevant Mannequin Challenge Dataset performs as well as the 2020 MPI model does on real estate data, when it comes to accurately predicting MPIs and disparity maps of video-chat-relevant frames involving close up shots of one or more persons.

\paragraph{Hypothesis B:}
If hypothesis A is disproved\footnote{Please refer to the Experiments and Results Chapter.}, the recreated MPI model should perform as well as the 2020 MPI model if the Mannequin Challenge training data were extended by the RealEstate10K dataset.

\input{chapters/ch3-methods/sec2-data}
\input{chapters/ch3-methods/sec3-implementation}

