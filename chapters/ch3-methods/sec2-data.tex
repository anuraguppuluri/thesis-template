\section{Data}\label{sec2:data} 

The steps taken to get both Mannequin Challenge and RealEstate10K datasets ready for training are as follows:


errors in our retraining attempts despite the authors of the preexisting model indicating in their paper that the model could be trained on a hodgepohdge of multiple resolutions   


\begin{itemize}
    \item Each dataset from the URLs listed in \cite{zhou2018stereo} and \cite{li2019learning} was downloaded into the current working directory. Both these datasets consist of .txt files pertaining to each video that can be downloaded. Each .txt file begins with the downloadable video’s YouTube link on the first line. And, from the second line onwards, it continues with listing the details of ORB-SLAM2 processed frames from the video \footnote{with one line for each frame} --- like the timestamp (in microseconds), camera intrinsics, and camera extrinsics.
    \item The GitHub repository associated with this thesis was downloaded from \url{https://github.com/au001/view-synthesis.git} into the working directory.
    \item The project's comprehensive Dockerfile was built from within the cloned repository by running \textit{build-run-docker/build-docker.sh}. Considerable time was spent to resolve dependency-version compatibilities by consulting the build log file whenever Dockerfile failed to build. After a successful build, the final docker container was started with \textit{build-run-docker/build-docker.sh}.
    \item {\sloppy Back inside either downloaded dataset directory, we ran the script \textit{../view-synthesis/scripts/get\_videos.py} on \textit{train/} folder to download all videos with youtube-dl at 720p resolution. Alternatively run \textit{../view-synthesis/scripts/get\_videos\_aria2c.py} to bolster youtube-dl’s download speed with Aria2 download manager. Standard output was saved to a log file to address possible download errors.}  
    \item Inevitably youtube-dl would throw download errors on the first run as there would be some partial and/or skipped downloads for various reasons ranging from the videos being taken down from YouTube over time to fixable errors intrinsic to youtube-dl. Moreover, some videos were unavailable in their 720p versions and were with the aim of maintaining consistency. Although scaled videos should not pose any problem to the training or the 3D point generation with COLMAP, we proceeded to attempt something different from the 2020 MPI authors, assuming the permitting scaled videos in their pipeline.
    \item As of this writing 2663 of $\sim$3000 Mannequin Challenge videos and 67582 of $\sim$70000 RealEstate10K videos were downloadable at 720p resolution and with no download errors on the first attempt. Hence, it became imperative to also save all the outputs generated by running the previous script to a log file and work upon it fix all download issues.
\end{itemize}

Descriptions of Mannequin Dataset, Real Estate dataset, COLMAP processing pipeline, data loader ---

COLMAP
There is a correspondence between the camera location C in world coordinates the distsnce lambda from C to the objerved 3D world point in the scene along the viewing ray of a projected pixel x on the imaging 

COLMAP is a 3D scene reconstruction pipeline. It attempts to recover the 3D scene structure from unstructured 2D images of the scene that come with no prior knowledge of the camera intrinsics, extrinsics, and nature of objects captured in the image. 
The extracted scene structure is either in the form of sparse 3D points along with the camera parameters for each input 2D image or in the form of dense 3D points with associated color information.

feature detection ---> pairwise feature matchng ---> correspondense estimation ---> incremental structure from motion

We had to make sure that we subejct videos to colmap procesing only after ensuring that ther 720p version og them were donloaded and for videos that were missed during handling of these large datasets we hasd to make sure that after properluy redownlioafuinhg the video again we colmap processed ti again 




