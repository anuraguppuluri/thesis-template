\section{Data}\label{sec:data} 

The steps taken to get both Mannequin Challenge~\cite{li2019learning} and RealEstate10K~\cite{zhou2018stereo} datasets ready for training are as follows:

\begin{itemize}

    \item Both these datasets consist of text files pertaining to each video. Each text file begins with the downloadable video’s YouTube link on the first line. And, from the second line onward, it continues with listing the details of ORB-SLAM2 and COLMAP processed frames from the video \footnote{with one line for each frame} --- such as the timestamp (in microseconds), camera intrinsics, and camera extrinsics.
    
    \item Each dataset from the URLs listed in \cite{zhou2018stereo} and \cite{li2019learning} was downloaded. 
    
    \item The GitHub repository associated with this thesis was downloaded from \url{https://github.com/au001/view-synthesis.git} into the working directory.

    \item The project's comprehensive Dockerfile was built from within the cloned repository by running \textit{build-run-docker/build-docker.sh}. Considerable time was spent to resolve dependency-version compatibilities by consulting the build log file whenever Dockerfile failed to build. After a successful build, the final docker container was started with \textit{build-run-docker/build-docker.sh}.
    
    \item {\sloppy Back inside either downloaded dataset directory, we ran the script \textit{../view-synthesis/scripts/get\_videos.py} on \textit{train/} folder to download all videos with youtube-dl at 720p resolution. Alternatively run \textit{../view-synthesis/scripts/get\_videos\_aria2c.py} to bolster youtube-dl’s download speed with Aria2 download manager. Standard output was saved to a log file to address possible download errors.}  
    
    \item Inevitably youtube-dl would throw download errors on the first run as there would be some partial and/or skipped downloads for various reasons ranging from the videos being taken down from YouTube over time to fixable errors intrinsic to youtube-dl. Moreover, some videos were unavailable in their 720p versions and were with the aim of maintaining consistency. Although scaled videos should not pose any problem to the training or the 3D point generation with COLMAP, we proceeded to attempt something different from the 2020 MPI authors, assuming the permitting scaled videos in their pipeline.
    
    \item As of this writing 2663 of $\sim$3000 Mannequin Challenge videos and 67582 of $\sim$70000 RealEstate10K videos were downloadable at 720p resolution and with no download errors on the first attempt. Hence, it became imperative to also save all the outputs generated by running the previous script to a log file and work upon it fix all download issues.
    of the available videos only so many were actually used colmap processed and used for training 
    The difference in numbers is attributed to availability of videos and COLMAP processing requirements not being met for some of them.

% Step 3: Extract to a new log file the lines from the previously saved log file that contain specific combinations of the strings “error:” and ": Downloading webpage" Run get_errors.py on get-vid-log-train-202106241546.txt
% python3 ../scripts/get_errors.py get-vid-log-train-202106241546.txt train-errors-202106281624.txt
% Step 3: Find the names of all the youtube online videos in <train-errors-202106281624.txt> that can be found in this specific pattern
% "youtube\] (.*?)\: Download"
% python3 ../scripts/get_name_within_pattern.py train-errors-202106281624.txt train-error-names-202106281635.txt
% Step 3 can only be executed if there are no .part mp4 downloads remaining in <train/> <test/> or <val/> folders
    
    
    
    % \item Similarities between MannequinChallenge and RealEstate10K datasets
    
    
\end{itemize}







% training we require triplets of images together with their relative
% camera poses and intrinsics.
% absolute camera poses are not required

% Visual SLAM and structure-from-motion have no way of determining absolute scale without external information:
% each of our training sequences is therefore equally valid
% if we scale the world (including the sparse point sets and
% the translation part of the camera poses) up or down by
% any constant factor. This is not an issue when dealing with
% multiple-image input since the relative pose between the
% inputs resolves the scale ambiguity, but it poses a challenge for learning any sort of 3D representation from a single input.

 
% /include{figure} training pipeline 
% draw.io


% inputs and outputs for all components 
% dataset text files dowload -----> youtubr downloader downloaded videos -------> COLMAP 3d points ------> 
%                                         |
%                                         |
%                                         |-------------------------------------------------------------->



Colmap is taking approx. 20 videos to process 1.5 hrs 
So a total of 67582 videos in train1 alone will take approx. 5068.5 hrs = 211 days.

% Descriptions of Mannequin Dataset, Real Estate dataset, COLMAP processing pipeline, data loader ---

% COLMAP
There is a correspondence between the camera location C in world coordinates the distance lambda from C to the observed 3D world point in the scene along the viewing ray of a projected pixel x on the imaging. 

COLMAP is a 3D scene reconstruction pipeline. It attempts to recover the 3D scene structure from unstructured 2D images of the scene that come with no prior knowledge of the camera intrinsics, extrinsics, and nature of objects captured in the image. The extracted scene structure is either in the form of sparse 3D points along with the camera parameters for each input 2D image or in the form of dense 3D points with associated color information.

feature detection --- pairwise feature matching --- correspondense estimation --- incremental structure from motion

We had to make sure that we subject videos to COLMAP processing only after ensuring that their 720p version of them were downloaded and for videos that were missed during handling of these large datasets we had to make sure that after properly redownloaded the video again we COLMAP processed it again. 


% Expanding upon pipeline description ---

% video ------------> input 1 --------------> training
% |
% |
% |-----------------> COLMAP -----------> 3D points per video frame -------------> input 2 -----------> same training

% how does data loader work?

% Original authors choice with colmap 
% my reason for pursuing colmap
% what does colmap do> --------> bundle adjustment, triangulation, 

% COLMAP highlevel

% Triangulation
% Bundle Adjustment

% data section

% for the 2018 paper
% During training, the images and
% MPI have a spatial resolution of 1024 × 576, but the model can be
% applied to arbitrary resolution at test time in a fully-convolutional
% manner.
% but maintaining resolution for 2020 paper is not required 

% 4.3 Refining poses with bundle adjustment
% We next process each sequence at higher resolution, using a standard
% structure-from-motion pipeline to extract features from each
% frame, match these features across frames, and perform a global
% bundle adjustment using the Ceres non-linear least squares optimizer
% [Agarwal et al. 2016].

% errors in our retraining attempts despite the authors of the preexisting model indicating in their paper that the model could be trained on a hodgepohdge of multiple resolutions   














