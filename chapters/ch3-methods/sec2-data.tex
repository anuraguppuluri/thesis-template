\section{Data}\label{sec:data} 

Both Mannequin Challenge~\cite{li2019learning} and RealEstate10K~\cite{zhou2018stereo} datasets were created by the same group of researchers hailing mostly from Google. Hence, they were created by the same ORBSLAM-2 processes and COLMAP processed and their text files list the camera parametr nd etc
MannequinChallenge and RealEstate10K datasets are similar in every way except for the genre of the videos themselves. Both these datasets consist of text files pertaining to each video. Each text file begins with the downloadable video’s YouTube link on the first line. And, from the second line onward, it continues with listing the details of ORB-SLAM2 and COLMAP processed frames from the video \footnote{with one line for each frame} --- such as the timestamp (in microseconds), camera intrinsics, and camera extrinsics.
    
Each dataset from the URLs listed in \cite{zhou2018stereo} and \cite{li2019learning} was downloaded. 
    
The \href{https://github.com/au001/view-synthesis.git}{\texttt{GitHub repository}} associated with this thesis was downloaded into the working directory.

The project's comprehensive Dockerfile was built from within the cloned repository by running \texttt{build-run-docker/build-docker.sh}. Considerable time was spent to resolve dependency-version compatibilities by consulting the build log file whenever Dockerfile failed to build. After a successful build, the final docker container was started with \texttt{build-run-docker/build-docker.sh}.
    
Back inside either downloaded dataset directory, we ran the script given by the path \texttt{../view-synthesis/scripts/get\_videos.py} on \texttt{train/} folder to download all videos with youtube-dl at 720p resolution. Alternatively we could run the script \texttt{../view-synthesis/scripts/get\_videos\_aria2c.py} to bolster youtube-dl’s download speed with Aria2 download manager. Standard output was saved to a log file to address possible download errors. 
    
Inevitably youtube-dl would throw download errors on the first run as there would be some partial and/or skipped downloads for various reasons ranging from the videos being taken down from YouTube over time to fixable errors intrinsic to youtube-dl. Moreover, some videos were unavailable in their 720p versions and were discarded with the aim of maintaining consistency. Although differently scaled videos should not pose any problem to the training or the 3D point generation with COLMAP, we proceeded to attempt something different from the single-view MPI authors, assuming the permitting scaled videos in their pipeline.
    
As of this writing 2663 of $\sim$3000 MannequinChallenge videos and 67582 of $\sim$70000 RealEstate10K videos were downloadable at 720p resolution and with no download errors on the first attempt. Hence, it became imperative to also save all the outputs generated by running the previous script to a log file and work upon it fix all download issues. Of the available RealEstate10K videos only $\sim$60000 were actually COLMAP-processed and used for training. The difference in numbers is attributed to unavailability of videos for some and COLMAP processing requirements not being met for some others.
    
COLMAP takes approximately 1.5 hours to process 20 videos if using CPU processing power alone. So a total of 67582 training videos would have taken approximately 5068.5 hrs or 211 days. Luckily for us, we were able to avail the benefits of Nvidia Tesla V100 GPUS (rated the best server model in 2020), and could bring down the processing time to 40 minutes for 20 videos.
    
In this way, we obtained the required points clouds and frames for both training and testing.

% Step 3: Extract to a new log file the lines from the previously saved log file that contain specific combinations of the strings “error:” and ": Downloading webpage" Run get_errors.py on get-vid-log-train-202106241546.txt
% python3 ../scripts/get_errors.py get-vid-log-train-202106241546.txt train-errors-202106281624.txt
% Step 3: Find the names of all the youtube online videos in <train-errors-202106281624.txt> that can be found in this specific pattern
% "youtube\] (.*?)\: Download"
% python3 ../scripts/get_name_within_pattern.py train-errors-202106281624.txt train-error-names-202106281635.txt
% Step 3 can only be executed if there are no .part mp4 downloads remaining in <train/> <test/> or <val/> folders


% maybe talk to them about the origins of mannequin challenge at jacksonville highschool and scientist say that ease of access is also probably why it became famous - 2016
% this was actually mannequin challenge version 1 hope the get to extend their dataset just like how real estate was able to be extended




% training we require triplets of images together with their relative
% camera poses and intrinsics.
% absolute camera poses are not required

% Visual SLAM and structure-from-motion have no way of determining absolute scale without external information:
% each of our training sequences is therefore equally valid
% if we scale the world (including the sparse point sets and
% the translation part of the camera poses) up or down by
% any constant factor. This is not an issue when dealing with
% multiple-image input since the relative pose between the
% inputs resolves the scale ambiguity, but it poses a challenge for learning any sort of 3D representation from a single input.

% Descriptions of Mannequin Dataset, Real Estate dataset, COLMAP processing pipeline, data loader ---

% COLMAP
As mentioned in previous chapters, there is a correspondence between the camera location $c$ in world coordinates the distance $\lambda$ from $c$ to the observed 3D world point in the scene along the viewing ray of a projected pixel $x$ on the imaging. COLMAP is a 3D scene reconstruction pipeline. It attempts to recover the 3D scene structure from unstructured 2D images of the scene that come with no prior knowledge of the camera intrinsics, extrinsics, and nature of objects captured in the image. The extracted scene structure is either in the form of sparse 3D points along with the camera parameters for each input 2D image or in the form of dense 3D points with associated color information. The pipeline consists of: feature detection $\rightarrow$ pairwise feature matching  $\rightarrow$ correspondence estimation $\rightarrow$ incremental structure from motion. We had to make sure that we subject videos to COLMAP processing only after ensuring that their 720p version of them were downloaded and for videos that were missed during handling of these large datasets we had to make sure that after properly redownloaded the video again we COLMAP processed it again.

% how does data loader work?

% Original authors choice with colmap 
% my reason for pursuing colmap
% what does colmap do> --------> bundle adjustment, triangulation, 

% COLMAP highlevel

% Triangulation
% Bundle Adjustment

% data section

% for the 2018 paper
% During training, the images and
% MPI have a spatial resolution of 1024 × 576, but the model can be
% applied to arbitrary resolution at test time in a fully-convolutional
% manner.
% but maintaining resolution for 2020 paper is not required 

% 4.3 Refining poses with bundle adjustment
% We next process each sequence at higher resolution, using a standard
% structure-from-motion pipeline to extract features from each
% frame, match these features across frames, and perform a global
% bundle adjustment using the Ceres non-linear least squares optimizer
% [Agarwal et al. 2016].

% errors in our retraining attempts despite the authors of the preexisting model indicating in their paper that the model could be trained on a hodgepohdge of multiple resolutions   














