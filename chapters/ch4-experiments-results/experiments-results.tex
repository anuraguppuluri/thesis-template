\chapter{Experiments and Results}\label{ch4:experiments-results}

In this chapter, we present some quantitative and qualitative evaluations of the variants of the recreated single-view MPI model retrained on various combinations of MannequinChallenge~\cite{li2019learning} and RealEstate10K~\cite{zhou2018stereo} datasets. We use the pretrained weights of the single-view MPI model as the benchmark and compare the abilities of all model variants at hand to generate novel views. We adopt some of the quantitative metrics from Tucker and Snavely's single-view MPI paper~\cite{single_view_mpi} --- PSNR, SSMI~\cite{wang_image_2004}, and LPIPS~\cite{zhang_unreasonable_2018} --- to give numeric values to the similarities between MPI-rendered video frames and the corresponding ground truth target frames the rendering process attempts to replicate. These metrics are computed over all image pixels at a time during evaluation. We based some of our metrics-evaluation scripts on TensorFlow Official Documentation~\cite{noauthor_tfimagepsnr_nodate,noauthor_tfimagessim_nodate}.

The model variants used to compute the metrics stated above are characterized by the following hyperparameters/metadata:
\begin{itemize}
    \item Depth loss weight, as explained in subsection~\ref{subsec:base-papers}.
    \item The number of disparity map channels specified in the \textit{tf.function} input signature for the bilinear sampling function in our training script (Appendix~\ref{sec:code-sources}),\\\textit{sample\_disparities(disparity,\ points)}, involving the predicted disparity and the input visible points.
    \item The lower bound on the number of visible points required per frame; videos with even one frame having the number of visible points below this threshold would be removed from training.
    \item The choice of datasets used to train on --- MannequinChallenge, RealEstate10K, or both.
    \item Whether multiple GPU workers were engaged or not.
\end{itemize}
Even seemingly innocuous hyperparameter values such as those for the number of disparity map channels specified, we believe, could have easily held sway over training progress. Pitting these variants against each other in the three computed metrics helped us select the best variant to simulate half a video chat with each time.

We manually sifted through the in-built test set of the MannequinChallenge dataset to handpick a set of 333 videos. These ORB-SLAM2-curated sequences had video-chat-relevant features. They mostly had the heads and torsos of people being focused upon rather than having wide shots of entire bodies. It was mostly just one or two people in the frames instead of several. Moreover, although not a strict requirement, the head pose of people in these frames was roughly or even very loosely aligned with the camera. There was hardly anybody in any frame that appeared to be looking directly at the camera, as might be expected in an actual video chat. We put these cherry-picked frames in the \textit{test-yes/} bin. We also curated \textit{test-maybe/} and \textit{test-no/} bins. They consisted of the rest of the MannequinChallenge test set with sequences either having no relevance to typical video chat settings (like there being hardly anyone in the frames) in the case of the \textit{test-no/} directory or falling heavily in the gray areas between \textit{test-yes/} and \textit{test-no/} in the case of the \textit{test-maybe/} folder. We even occasionally interspersed the \textit{test-yes/} and \textit{test-maybe/} bins with videos containing sequences that portrayed people facing diametrically opposite the camera. This was just so that we could really challenge the model variant being tested. Table~\ref{tab:video-classifications} shows the classifications of procured videos.

\newcolumntype{L}{>{\raggedright\arraybackslash}p{0.275\textwidth}}
\newcolumntype{M}{>{\raggedright\arraybackslash}p{0.17\textwidth}}
\newcolumntype{N}{>{\raggedleft\arraybackslash}p{0.12\textwidth}}

\begin{table*}[t]
    \centering
    \begin{tabular}{L|M|N|N}
    \toprule
  
    \textbf{Dataset} & \textbf{Bin} & \textbf{Videos} & \textbf{Frames} \\
    
    \midrule
    
    RealEstate10K & \textit{train/} & 66861 & 9095528 \\

    \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4}
    
    MannequinChallenge & \textit{train/} & 2364 & 117811 \\
    
    \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4}
    
    MannequinChallenge & \textit{validation/} & 88 & 5928 \\
    
    \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4}
    
    MannequinChallenge & \textit{test-yes/} & 333 & 12595 \\
    
    \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4}
    
    MannequinChallenge & \textit{test-maybe/} & 300 & 12831 \\
    
    \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4}
    
    MannequinChallenge & \textit{test-no/} & 24 & 728 \\
    
    \bottomrule
    \end{tabular}
    \caption{Classifications of Procured Videos}
    \label{tab:video-classifications}
    {\small Enormous quantities of data help reduce overfitting. Also, subjecting the model to various indoor and outdoor scenes and image collections used in prior work helps make it robust and generalize well even in the presence of scene anomalies.}
\end{table*}

Of the various aspects of the code that we modeled from the textual descriptions and relevant code snippets obtained from both the single-view and stereo MPI papers, such as \textit{generator\_wandb.py}, \textit{data\_loader.py}, \textit{train\_wandb.py}, and \textit{test.py}, the scripts relevant to the experiments in this section are \textit{test.py} and \textit{generator\_wandb.py} (Appendix~\ref{sec:code-sources}). For testing, the generator first aggregates all video names from the directory input to it, and for each of these, it picks various \textit{reference\_image} and \textit{target\_image} pairs that are internally 5 frames apart. \textit{reference\_image} is the frame \textit{test.py} uses to infer the MPI representation of the scene from, and \textit{target\_image} is supposedly a view of the same scene from a different angle. The possibility that, when the camera moves from one scene to another in the same video, \textit{reference\_image} may depict a scene different from the one captured by \textit{target\_image} is expected to be extremely low as both datasets have been curated by similar ORB-SLAM2 and COLMAP processes. In such hypothetical cases, \textit{target\_image} will be erroneously rendered by \textit{mpi.render} function as the corresponding \textit{rendered\_image}. Nevertheless, since we take the mean of the computed metrics over hundreds of \textit{test.py} processed \textit{reference\_image}-\textit{target\_image} pairs, we believe the final accuracies of a variant's mean metrics will not be off the mark much and shall still be used to determine a variant’s performance satisfactorily. Each of the three metrics is calculated between \textit{target\_image} and \textit{rendered\_image}, which are situated 5 frames apart along the camera trajectory of the respective clip. We did not repeat the same test process for frames 10 apart, which would just have been done to show\footnote{as in the case of the single-view MPI paper} that the longer the baseline between reference/source and target views, the less the rendered image’s accuracy will be. On the same note, we have also not calculated the metrics internally for all processed \textit{(reference\_image, target\_image)} pairs, which would just have been done to catch the hypothetical anomalies of the complete scene changes within a clip, as mentioned before. 

We also took an interesting little detour in our project when we attempted to parallelize training across multiple GPUs. We believe this would have allowed us to increase the batch size\footnote{currently limited to 4 pairs of reference and target images and their respective camera poses and intrinsics, along with the 3D points of the reference image} and thereby let larger and larger parts of our 60000+ training-ready sequences with associated point clouds be used for learning by our recreated model. This would have assisted the model in better avoiding local minima and maxima. But, since TensorFlow's direct conversion procedure that would let standard single-GPU-utilizing scripts become multi-GPU-faring is as yet still an evolving process requiring careful attention to resource allocation issues among the various replicas of the parallelizable aspects of the model\footnote{such as the dataset generator, the loss functions aggregator, etc.} spread across GPUs, our training got undercut after a good start by a resource exhaustion error at training step 178. Nevertheless, we computed all three metrics for this other model variant retrained on MannequinChallenge data using \textit{tf.distribute.MirroredStrategy}, capable of harnessing the power of multiple GPUs.

\begin{figure}[!h]
    \begin{tabular}{cccc}

        \subfloat[Disparity Map --- gallant-eon-27]{\includegraphics[width = 1.3in]{figures/gallant-eon-27/gallant-eon-27-disparity-step1.png}} &
        \subfloat[Disparity Map --- gallant-eon-27]{\includegraphics[width = 1.3in]{figures/gallant-eon-27/gallant-eon-27-disparity-step1750.png}} &
        \subfloat[Disparity Map --- gallant-eon-27]{\includegraphics[width = 1.3in]{figures/gallant-eon-27/gallant-eon-27-disparity-step14000.png}} &
        \subfloat[Disparity Map --- gallant-eon-27]{\includegraphics[width = 1.3in]{figures/gallant-eon-27/gallant-eon-27-disparity-step15250.png}}\\
        
        \subfloat[Rerendered Target --- gallant-eon-27]{\includegraphics[width = 1.3in]{figures/gallant-eon-27/gallant-eon-27-rendered-step1.png}} &
        \subfloat[Rerendered Target --- gallant-eon-27]{\includegraphics[width = 1.3in]{figures/gallant-eon-27/gallant-eon-27-rendered-step1750.png}} &
        \subfloat[Rerendered Target --- gallant-eon-27]{\includegraphics[width = 1.3in]{figures/gallant-eon-27/gallant-eon-27-rendered-step14000.png}} &
        \subfloat[Rerendered Target --- gallant-eon-27]{\includegraphics[width = 1.3in]{figures/gallant-eon-27/gallant-eon-27-rendered-step15250.png}}\\
        
        \subfloat[Disparity Map --- sunny-grass-5]{\includegraphics[width = 1.3in]{figures/sunny-grass-5/sunny-grass-5-disparity-step0.png}} &
        \subfloat[Disparity Map --- sunny-grass-5]{\includegraphics[width = 1.3in]{figures/sunny-grass-5/sunny-grass-5-disparity-step1750.png}} &
        \subfloat[Disparity Map --- sunny-grass-5]{\includegraphics[width = 1.3in]{figures/sunny-grass-5/sunny-grass-5-disparity-step14000.png}} &
        \subfloat[Disparity Map --- sunny-grass-5]{\includegraphics[width = 1.3in]{figures/sunny-grass-5/sunny-grass-5-disparity-step15250.png}}\\
        
        \subfloat[Rerendered Target --- sunny-grass-5]{\includegraphics[width = 1.3in]{figures/sunny-grass-5/sunny-grass-5-rendered-step0.png}} &
        \subfloat[Rerendered Target --- sunny-grass-5]{\includegraphics[width = 1.3in]{figures/sunny-grass-5/sunny-grass-5-rendered-step1750.png}} &
        \subfloat[Rerendered Target --- sunny-grass-5]{\includegraphics[width = 1.3in]{figures/sunny-grass-5/sunny-grass-5-rendered-step14000.png}} &
        \subfloat[Rerendered Target --- sunny-grass-5]{\includegraphics[width = 1.3in]{figures/sunny-grass-5/sunny-grass-5-rendered-step15250.png}}\\

    \end{tabular}
    \caption{Some Variants' Disparity Maps Turn Gray Faster Than Others}
    \label{fig:gray-disparity-maps}
    {\small Outputs from left to right: for step 1, for step 1750, for step 14000, for step 15250. Source of frames shown: MannequinChallenge dataset~\cite{li2019learning}.}
\end{figure}

The rest of this chapter presents the results of the experiments done with the various model variants and the baseline pretrained model. We then cap it off by presenting the results of incorporating OpenFace 2.2 into the inference pipeline. As of this writing, our generator is only able to pick random pairs of reference and target frames from the 333 \textit{test-yes/} videos. Sequential pair-picking would avoid possible repetition of selected pairs and allow for exhaustive coverage of the test set. Given that even the smaller one of the two datasets has 100000+ frames and that we have not been able to resolve the issue of the synthesized disparity maps becoming smudgier and smudgier until they turn entirely gray/monochromatic even before some variants could hit 14000 training steps (Figure~\ref{fig:gray-disparity-maps})), it is not very likely that the model may see the same frame twice. So perhaps, computing evaluation metrics with training data can double in as doing the same with validation data itself, even though we haven't set aside validation data. As for the metrics, an LPIPS value of 0 indicates there is either a perfect match between the images being compared or the images being compared are the same. On the other hand, SSIM values of 1 indicate a perfect match. Both these metrics range from 0 to 1. PSNR values, measured in decibels (dB), don't generally have an upper limit, but values 20 dB and higher are considered acceptable. In calibrating our implementations of these metrics, when we compared an image with itself, we found the mean LPIPS, SSIM, and PSNR values over 300 images to be close to 0, 1, and greater than 30, respectively.

\begin{figure}[!h]
    \includegraphics[width=1\columnwidth]{figures/model-variants-metrics.png}
    \caption{Model Variants' Mean PSNR, SSIM, and LPIPS Evaluation Values Over 300 Testing Instances}
    \label{fig:model-variants-metrics}
\end{figure}

We can see how the variants stack up against one another in figure~\ref{fig:model-variants-metrics}. Perceptual similarity comes closest to how humans judge an image's picture quality. Hence, we chose the variant northern-monkey-4 for the final step of simulating a video chat. These catchy names are automatically allotted by wandb.ai at the start of any training run. If the run is relatively successful, we use the final model produced by it as one of our variants and evaluate its performance. All our variants have been trained to the limit and to the point where the loss becomes less than 1, after having come down all the way from 1188 and starting to stagnate. This has invariably occurred sooner than 25000 training steps for all our variants (Figure~\ref{fig:mean-loss}). This goes to show that had we been entirely successful in our implementation of the model, we would also have been able to train for way more than 100000 steps\footnote{similarly to Tucker and Snavely~\cite{single_view_mpi}}. Besides, we couldn't really compute the accuracy metric for our models and variants to complement the loss metric as we're training in steps/batches, not epochs.

\begin{figure}[!h]
    \includegraphics[width=0.75\columnwidth]{figures/mean-loss.png}
    \caption{Typical Mean Loss Chart for Any of Our Training Runs}
    \label{fig:mean-loss}
    {\small wandb.ai somehow always shows twice the number of actual training steps completed on our server. Hence all our variants' training stagnates at 30,000+ steps and not at the 60,000+ steps shown in this wandb.ai-logged loss chart.}
\end{figure}

\begin{figure}[!h]
    \begin{tabular}{cccc}

        \subfloat[]{\includegraphics[width = 1.3in]{figures/baseline/000001_image_disparity.png}} &
        \subfloat[PSNR $\uparrow$ Target vs Rendered = 12.345]{\includegraphics[width = 1.3in]{figures/baseline/000001_image_reference.png}} &
        \subfloat[SSIM $\uparrow$ Target vs Rendered = 0.509]{\includegraphics[width = 1.3in]{figures/baseline/000001_image_target.png}} &
        \subfloat[LPIPS $\downarrow$ Target vs Rendered = 0.520]{\includegraphics[width = 1.3in]{figures/baseline/000001_image_render.png}}\\
        
        \subfloat[]{\includegraphics[width = 1.3in]{figures/gallant-eon-27/000001_image_disparity.png}} &
        \subfloat[PSNR $\uparrow$ Target vs Rendered = 12.300]{\includegraphics[width = 1.3in]{figures/gallant-eon-27/000001_image_reference.png}} &
        \subfloat[SSIM $\uparrow$ Target vs Rendered = 0.470]{\includegraphics[width = 1.3in]{figures/gallant-eon-27/000001_image_target.png}} &
        \subfloat[LPIPS $\downarrow$ Target vs Rendered = 0.338]{\includegraphics[width = 1.3in]{figures/gallant-eon-27/000001_image_render.png}}\\
        
        \subfloat[]{\includegraphics[width = 1.3in]{figures/giddy-microwave-29/000001_image_disparity.png}} &
        \subfloat[PSNR $\uparrow$ Target vs Rendered = 12.282]{\includegraphics[width = 1.3in]{figures/giddy-microwave-29/000001_image_reference.png}} &
        \subfloat[SSIM $\uparrow$ Target vs Rendered = 0.470]{\includegraphics[width = 1.3in]{figures/giddy-microwave-29/000001_image_target.png}} &
        \subfloat[LPIPS $\downarrow$ Target vs Rendered = 0.338]{\includegraphics[width = 1.3in]{figures/giddy-microwave-29/000001_image_render.png}}

    \end{tabular}
    \caption{Baseline and (MannequinChallenge + RealEstate10K)-Based Model Variants' Output Visualizations With a MannequinChallenge Target Frame}
    \label{fig:output-visualizations-1}
    {\small Variants from top to bottom: baseline, gallant-eon-27, giddy-microwave-29.\\Outputs from left to right: disparity map, reference frame, target frame, rerendered target. Source of frames shown: MannequinChallenge dataset~\cite{li2019learning}.}
\end{figure}

What further validates our choice of northern-monkey-4 is the set of output visualizations for all relatively successful model variants shown in figures~\ref{fig:output-visualizations-1} and~\ref{fig:output-visualizations-2}. These outputs further reveal that even prior to all our fine-tuning, the pretrained model found it hard to synthesize the disparity for video-chat-relevant frames. In the sample test frame used in these figures, the person has clearly moved closer to the camera, but the frame synthesized by the baseline model shows ``stack of cards" effects. This could potentially also be the reason that while the picture quality for the renderings seems to have been greatly improved by our fine-tuning\footnote{as is evident from the improved LPIPS values}, the already nebulous disparity synthesis (when it comes to video chat frames) has been rendered asunder. It also stands to reason that perhaps depth/disparity is taken more into account by the SSIM metric than by the other two metrics, owing to the stark decrease in SSIM values for the fine-tuned variants. It is structural similarity, after all, and we have already established that depth is part of the 3D structure of the scene. Thus, we have been further validated in our efforts to retrain the baseline model in the first place.

However, even though the picture quality of the outputs of our fine-tuned variants has greatly improved, yet only one of the 32 MPI layers is able to pick up\footnote{capture details from} each input frame, as opposed to how all 32 MPI layers of the pretrained baseline are able to pick up some part of the input frame or the other based on the (incorrectly (Chapter~\ref{ch3:methods})) inferred depth (Figure~\ref{fig:mpi-layers-picked-up}). We theorize that this could directly be related to the disparity maps generated by our variants for each input frame becoming progressively more monochromatic instead of progressively sharper in detail, which would have meant the model was starting to surpass expectations on all levels, performance-wise. 

\begin{figure}[!h]
    \begin{tabular}{cccc}

        \subfloat[]{\includegraphics[width = 1.3in]{figures/northern-monkey-4/000001_image_disparity.png}} &
        \subfloat[PSNR $\uparrow$ Target vs Rendered = 12.315]{\includegraphics[width = 1.3in]{figures/northern-monkey-4/000001_image_reference.png}} &
        \subfloat[SSIM $\uparrow$ Target vs Rendered = 0.470]{\includegraphics[width = 1.3in]{figures/northern-monkey-4/000001_image_target.png}} &
        \subfloat[LPIPS $\downarrow$ Target vs Rendered = 0.337]{\includegraphics[width = 1.3in]{figures/northern-monkey-4/000001_image_render.png}}\\
        
        \subfloat[]{\includegraphics[width = 1.3in]{figures/sunny-grass-5/000001_image_disparity.png}} &
        \subfloat[PSNR $\uparrow$ Target vs Rendered = 12.315]{\includegraphics[width = 1.3in]{figures/sunny-grass-5/000001_image_reference.png}} &
        \subfloat[SSIM $\uparrow$ Target vs Rendered = 0.470]{\includegraphics[width = 1.3in]{figures/sunny-grass-5/000001_image_target.png}} &
        \subfloat[LPIPS $\downarrow$ Target vs Rendered = 0.337]{\includegraphics[width = 1.3in]{figures/sunny-grass-5/000001_image_render.png}}\\
        
        \subfloat[]{\includegraphics[width = 1.3in]{figures/fast-monkey-7/000001_image_disparity.png}} &
        \subfloat[PSNR $\uparrow$ Target vs Rendered = 12.284]{\includegraphics[width = 1.3in]{figures/fast-monkey-7/000001_image_reference.png}} &
        \subfloat[SSIM $\uparrow$ Target vs Rendered = 0.471]{\includegraphics[width = 1.3in]{figures/fast-monkey-7/000001_image_target.png}} &
        \subfloat[LPIPS $\downarrow$ Target vs Rendered = 0.339]{\includegraphics[width = 1.3in]{figures/fast-monkey-7/000001_image_render.png}}\\

    \end{tabular}
    \caption{MannequinChallenge-Based Model Variants' Output Visualizations With a MannequinChallenge Target Frame}
    \label{fig:output-visualizations-2}
    {\small Variants from top to bottom: northern-monkey-4, sunny-grass-5, fast-monkey-7.\\Outputs from left to right: disparity map, reference frame, target frame, rerendered target. Source of frames shown: MannequinChallenge dataset~\cite{li2019learning}.}
\end{figure}

\begin{figure}[!h]
    \begin{tabular}{cccc}
        
        \subfloat[Pretrained Baseline Model's~\cite{single_view_mpi} MPI Layers When Processing Sample Frame]{\includegraphics[width = 0.9\linewidth]{figures/mpi-layers-viewer.png}}\\
        
        \subfloat[Variant northern-monkey-4's MPI Layers When Processing Sample Frame]{\includegraphics[width = 0.9\linewidth]{figures/mpi-layers-viewer-ours.png}}\\

    \end{tabular}
    \caption{Our Variants' MPI Layers Don't All Pick Up Each Input Frame, Whereas The Pretrained Baseline Model's Ones Do}
    \label{fig:mpi-layers-picked-up}
    {\small Source of frame shown: YouTube~\cite{jimmy_kimmel_live_we_2018}.}
\end{figure}

To complete the simulation of one-half of a video chat pipeline with the help of OpenFace 2.2, we have included a few video-chat-resembling frames of viewers and viewees (Figures~\ref{fig:pipeline-completion-sample-1}~and~\ref{fig:pipeline-completion-sample-2}) and shown how viewee frames rerendered from the perspective of the viewer vary with the viewer's changing head pose. Similarly, to simulate the other half of the pipeline, we have switched the roles of viewers and viewees and shown corresponding changes in respective frames occurring at the same timestamps as the first half. We encourage the reader to head over to the GitHub repository for this project for videos of said simultaneous changes occurring in a two-way video chat. If this pipeline were to be deployed in a manner consistent with what Google's Project Starline~\cite{lawrence_project_2021} has going for it, we believe a 3D version of the video chat, akin to Project Starline, would be simulated.

\begin{figure}[!h]
    \begin{tabular}{cccc}

        \subfloat[Viewer Frame --- OpenFace-Tracked]{\includegraphics[width = 1.3in]{figures/viewers/XYviM5xevC8-0002315_off_img00024.png}} &
        \subfloat[Viewee Frame --- northern-monkey-4-Rerendered]{\includegraphics[width = 1.3in, height = 1.3in]{figures/viewees/OVWb9aTUQmA-ours_frame00024.png}} &
        \subfloat[Viewer Frame --- OpenFace-Tracked]{\includegraphics[width = 1.3in]{figures/viewers/OVWb9aTUQmA-0003315_off_img00024.png}} &
        \subfloat[Viewee Frame --- northern-monkey-4-Rerendered]{\includegraphics[width = 1.3in, height = 1.3in]{figures/viewees/XYviM5xevC8-ours_frame00024.png}}\\
        
        \subfloat[Viewer Frame --- OpenFace-Tracked]{\includegraphics[width = 1.3in]{figures/viewers/XYviM5xevC8-0002315_off_img00056.png}} &
        \subfloat[Viewee Frame --- northern-monkey-4-Rerendered]{\includegraphics[width = 1.3in, height = 1.3in]{figures/viewees/OVWb9aTUQmA-ours_frame00056.png}} &
        \subfloat[Viewer Frame --- OpenFace-Tracked]{\includegraphics[width = 1.3in]{figures/viewers/OVWb9aTUQmA-0003315_off_img00056.png}} &
        \subfloat[Viewee Frame --- northern-monkey-4-Rerendered]{\includegraphics[width = 1.3in, height = 1.3in]{figures/viewees/XYviM5xevC8-ours_frame00056.png}}\\
        
        \subfloat[Viewer Frame --- OpenFace-Tracked]{\includegraphics[width = 1.3in]{figures/viewers/XYviM5xevC8-0002315_off_img00024.png}} &
        \subfloat[Viewee Frame --- Pretrained-Baseline-Rerendered]{\includegraphics[width = 1.3in, height = 1.3in]{figures/viewees/OVWb9aTUQmA-rendered_frame00024.png}} &
        \subfloat[Viewer Frame --- OpenFace-Tracked]{\includegraphics[width = 1.3in]{figures/viewers/OVWb9aTUQmA-0003315_off_img00024.png}} &
        \subfloat[Viewee Frame --- Pretrained-Baseline-Rerendered]{\includegraphics[width = 1.3in, height = 1.3in]{figures/viewees/XYviM5xevC8-rendered_frame00024.png}}\\

    \end{tabular}
    \caption{3D Video Chat Simulation Snapshots --- Sample I}
    \label{fig:pipeline-completion-sample-1}
    {\small When the viewer looks top-left, the viewee goes bottom-right, and vice versa. All frames in a row occur at the same timestamp. The rightmost columns show the same viewers and viewees as the leftmost ones, but with their roles switched. This entire bidirectional showcasing, technically, simulates an actual 3D video chat. Also, northern-monkey-4-rerendered frames look much sharper than pretrained-baseline-rerendered ones. Source of frames shown: YouTube~\cite{jimmy_kimmel_live_we_2018,theellenshow_adorable_2020}.}
\end{figure}

\begin{figure}[!h]
    \begin{tabular}{cccc}

        \subfloat[Viewer Frame --- OpenFace-Tracked]{\includegraphics[width = 1.3in]{figures/viewers/off_img00000.png}} &
        \subfloat[Viewee Frame --- northern-monkey-4-Rerendered]{\includegraphics[width = 1.3in, height = 1.3in]{figures/viewees/ours_img00000.png}} &
        \subfloat[Viewer Frame --- OpenFace-Tracked]{\includegraphics[width = 1.3in]{figures/viewers/off_img00016.png}} &
        \subfloat[Viewee Frame --- northern-monkey-4-Rerendered]{\includegraphics[width = 1.3in, height = 1.3in]{figures/viewees/ours_img00016.png}}\\
        
        \subfloat[Viewer Frame --- OpenFace-Tracked]{\includegraphics[width = 1.3in]{figures/viewers/off_img00021.png}} &
        \subfloat[Viewee Frame --- northern-monkey-4-Rerendered]{\includegraphics[width = 1.3in, height = 1.3in]{figures/viewees/ours_img00021.png}} &
        \subfloat[Viewer Frame --- OpenFace-Tracked]{\includegraphics[width = 1.3in]{figures/viewers/off_img00074.png}} &
        \subfloat[Viewee Frame --- northern-monkey-4-Rerendered]{\includegraphics[width = 1.3in, height = 1.3in]{figures/viewees/ours_img00074.png}}\\
        
        \subfloat[Viewer Frame --- OpenFace-Tracked]{\includegraphics[width = 1.3in]{figures/viewers/off_img00000.png}} &
        \subfloat[Viewee Frame --- Pretrained-Baseline-Rerendered]{\includegraphics[width = 1.3in, height = 1.3in]{figures/viewees/rendered_img00000.png}} &
        \subfloat[Viewer Frame --- OpenFace-Tracked]{\includegraphics[width = 1.3in]{figures/viewers/off_img00016.png}} &
        \subfloat[Viewee Frame --- Pretrained-Baseline-Rerendered]{\includegraphics[width = 1.3in, height = 1.3in]{figures/viewees/rendered_img00016.png}}\\

    \end{tabular}
    \caption{3D Video Chat Simulation Snapshots --- Sample II}
    \label{fig:pipeline-completion-sample-2}
    {\small The pipeline even covers cases of extreme viewer head rotations, as in the case of the girl in red performing some neck exercises. Moreover, we don't see ``stack of cards" effects in northern-monkey-4-rerendered frames primarily because not all of northern-monkey-4's MPI layers pick up the input frame, as shown in figure~\ref{fig:mpi-layers-picked-up}. Source of frames shown: YouTube~\cite{theellenshow_kid_2018,perfect_balance_clinic_-_pain_relief_specialists_head_2018}.}
\end{figure}

In reference to the qualitative results presented throughout this work, we invoke the reader to adopt Tucker and Snavely's~\cite{single_view_mpi} use of pointers, such as the handling of occluded content, the production of undesirable artifacts at the edges of foreground objects, and so on, to qualitatively compare the discrepancies in the results generated by each model variant. Similarly, visually checking for the accuracy of the synthesized disparity maps, as illustrated at the beginning of chapter~\ref{ch3:methods}, is also useful in verifying the quality of the MPIs produced. We encourage the reader to zoom into the electronic version of this thesis or take to the GitHub repository accompanying this work (Appendix~\ref{sec:code-sources}) for easier visual verification.