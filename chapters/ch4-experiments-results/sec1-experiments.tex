\chapter{Experiments and Results}\label{ch2:experiments-results}

\section{Experiments}\label{sec1:experiments} 

Here are some of the quantitative and qualitative evaluations of the variants of the recreated 2020 Single-View MPI model trained on different combinations of the Mannequin Challenge and RealEstate10K datasets. We use the pretrained weights of the 2020 MPI model as the benchmark and compare the abilities of all models at hand to generate novel views (more extensive training underway). We adopt some of the quantitative metrics from the 2020 MPI paper \cite{single_view_mpi} --- PSNR, SSMI \cite{wang_image_2004}, and LPIPS \cite{zhang_unreasonable_2018} --- to give numeric values to the similarities between MPI-rendered video frames and the corresponding ground truth target frames the rendering process attempts to replicate. 

The following are the model variants used to compute the metrics stated above to help provide arguments for or against each of the hypotheses stated in section \ref{sec1:approach}: ---
\begin{itemize}
    \item The pretrained weights of the 2020 single-view MPI model trained exclusively on RealEstate10K data.
    \item The recreated 2020 model retrained exclusively on the Mannequin Challenge dataset with transfer learning using the pretrained weights of the original 2020 MPI model. In this transfer learning process, none of the layers of the pretrained weights were frozen and so could learn and evolve based on the Mannequin Challenge data they were newly exposed to.
    \item The recreated 2020 model retrained not just on the Mannequin Challenge dataset but also on the RealEstate10K dataset with similar transfer learning as in the previous variant. This variant was encouraged to be tried out by one of the authors of the 2020 MPI paper in our correspondences with him. \cite{single_view_mpi}
\end{itemize}

We sifted through the test set of the Mannequin Challenge dataset to hand pick a set of 333 videos that contained ORB-SLAM2 recognized sequences\footnote{the timestamp, camera intrinsics and extrinsics of all frames of each of which are listed in the corresponding .txt files in the dataset} which had video-chat-relevant features like the head and torso of people being focused on rather than having wide shots of entire bodies, the number of people in the frame being mostly limited to one or two as opposed to multiple people being featured, and the head pose of people being roughly or even very loosely aligned with the camera (there was hardly anybody that looked directly at the camera). We put them in the test-yes/ bin. We also curated test-maybe/ (311 videos) and test-no/ (25 videos) bins that consisted of the rest of the Mannequin Challenge test set with sequences either having no relevance to video chat (like there being hardly anyone in the frames) for test-no/ to those falling heavily in the gray areas between test-yes/ and test-no/ for test-maybe/. We even occasionally interspersed the test-yes/ and test-maybe/ bins with videos containing sequences that portrayed people facing diametrically opposite the camera just to really challenge the model variant being tested.

{\sloppy Of the various aspects of the code that we modelled from the textual descriptions and relevant code snippets procured from both the 2020 Single-View and 2018 Stereo MPI papers\footnote{Please find our code repository at \url{https://github.com/au001/view-synthesis.git}} like \verb`generator_test.py`, \verb`generator_train.py`, \verb`data_loader.py`, \verb`train.py`, and \verb`test.py`, the scripts relevant to the experiments in this section are test.py and generator\_test.py. For testing, the generator first aggregates all videos names from the directory input to it and for each of these, it picks reference\_image and target\_image to be either 5 or 10 frames apart. reference\_image is the frame that test.py uses to infer the MPI of the scene from and target\_image is supposedly a view of the same scene from a different angle. The possibility that, when the camera moves from one scene to another in the same video, reference\_image may depict a scene different from the one captured by target\_image is expected to be extremely rare as both datasets have been curated by a similar ORB-SLAM2 process like COLMAP. In such hypothetical cases, target\_image will be erroneously rendered by mpi.render() in rendered\_image. But since we take the mean of the computed metrics over hundreds of test.py processed reference\_image, target\_image pairs, we believe the final accuracies of a variant's mean metrics will not be off the tracks much and that they shall still be used to determine a variantâ€™s performance satisfactorily. Each of the three metrics are calculated between target\_image and rendered\_image. We first test and compute metrics of frames 5 part and then we repeat the same test process for frames 10 apart just to show (as in the case of the 2020 MPI paper) that the longer the baseline between reference/source and target views, the less the accuracy will be of the rendered image. Moreover, we also calculate the metrics for all processed reference\_image, target\_image pairs, to catch the hypothetical anomalies of complete scene changes mentioned above.}  

Another jewel in our project was our attempt to parallelize training across multiple GPUs, which we believed would allow us to increase the batch size --- currently limited to 4 pairs of reference and target images along with their respective camera poses, intrinsics, and 3D points of the reference image --- and thereby let larger parts of our 70000+ training ready sequences with associated point clouds be used for learning by our recreated model. But, since TensorFlow's direct conversion procedure that would let standard single-GPU utilizing scripts become multi-GPU-faring is as yet still an evolving process requiring careful attention to resource allocation issues among the various replicas of the parallelizable aspects of the model --- like the dataset generator, the loss functions aggregator, etc. --- spread across GPUs, our training got undercut after a good start by the following error at training step 178: 

\begin{lstlisting}[breaklines]
    tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[1,32,512,512,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StridedSlice] name: render/compose_back_to_front/strided_slice/
\end{lstlisting}

Nevertheless, we computed all three metrics for the second model variant retrained on Mannequin Challenge data but this time with \verb`tf.distribute.MirroredStrategy`, harnessing the power of multiple GPUs.

\input{chapters/ch4-experiments-results/sec2-results}