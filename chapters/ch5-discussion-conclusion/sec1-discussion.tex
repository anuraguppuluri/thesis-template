\chapter{Discussion}\label{ch:discussion}

% from deep stereo paper the causes for overfitting

% Deep networks have enjoyed huge success in recent
% years, particularly for image understanding tasks [20, 29].
% Despite these successes, relatively little work exists on applying
% deep learning to computer graphics problems and especially
% to generating new views from real imagery. One
% possible reason is the perceived inability of deep networks
% to generate pixels directly, but recent work on denoising
% [35], super-resolution [6], and rendering [21] suggest
% that this is a misconception. Another common objection is
% that deep networks have a huge number of parameters and
% hence are prone to overfitting in the absence of enormous
% quantities of data, but recent work [29] has demonstrated
% state-of-the-art deep networks whose parameters number in
% the low millions, greatly reducing the potential for overfitting.

% from deep stereo on the success of neural nets

% In this work we present a new approach to new view synthesis
% that uses deep networks to regress directly to output
% pixel colors given the posed input images. Our system
% is able to interpolate between views separated by a
% wide baseline and exhibits resilience to traditional failure
% modes, including graceful degradation in the presence of
% scene motion and specularities. We posit this is due to the
% end-to-end nature of the training, and the ability of deep
% networks to learn extremely complex non-linear functions
% of their inputs [25].
% Additionally, although we focus on its application
% to new view problems here, we believe that the
% deep architecture presented can be readily applied to other
% stereo and graphics problems given suitable training data.
% Because
% of the variety of the scenes seen in training our system is robust
% and generalizes to indoor and outdoor imagery, as well
% as to image collections used in prior work.

Through this thesis, we had the opportunity to form one half of a 2-way pipeline that is able to render new views from the perspectives of both the participants in video chats. As hypothesized, the PSNR and SSIM metrics of the baseline model compared with the retrained model show that there is a slight improvement in the performance of a model trained exclusive on the non-video-chat-relevant real data over the much-more-video-chat relevant MannequinChallenge data. 

The qualities of the rendered image and the predicted MPI have been improved first by going from training exclusively on real estate data to going to exclusive training on mannequin data and then finally onto a mix of both. We complete the one-way part of a two-way, potentially real-time rendering pipeline that takes in the head pose of each ``viewer" video frame and renders the corresponding ``viewee" video frame i.e., the one that syncs with the timestamp of the ``viewer".

Although the sharpness of the rerendered images is twice as good with our chosen model variant as with the pretrained baseline, the predicted MPIs layers have all but collapsed to a single depth layer. This is also evident from the way the training would start to produce completely gray disparity maps from step 14000 onward, as noted in chapter~\ref{ch4:experiments-results}. We believe that the reason for this is more likely to be found in the weights we give to our various loss functions aggregating into a mean loss. Experimenting with taking out the pixel loss and bringing the smoothness loss way down would help to isolate the issue even more. Although we made our best efforts to reconstruct the loss function and the rest of training setup as close to the textual descriptions in the paper as possible, it would definitely shine a lot more light on the root cause of the problem if we were able to access the training script of the authors which had not been made publicly available by them. Also, since we were also meticulous with our data curation, we don't believe that it is likely that input data has any part to play in the generation of NaN loss errors.  

\input{chapters/ch5-discussion-conclusion/sec2-conclusion}
\input{chapters/ch5-discussion-conclusion/sec3-future-work}