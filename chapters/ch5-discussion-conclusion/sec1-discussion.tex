\chapter{Discussion}\label{ch:discussion}

Through this thesis, we have had the opportunity to simulate both halves of a 2-way pipeline that can render novel views from the perspectives of both participants in a video chat. Going by the synthesized monochromatic disparity maps and even by the SSIM values shown in chapter~\ref{ch4:experiments-results}, we found that our model variants struggled to synthesize disparity sufficiently well. Going by the LPIPS values, we found that they excelled at synthesizing the actual target view itself. This is indeed unexpected, given that only one of the available 32 MPI layers is able to, essentially, duplicate the reference image in its entirety. Further testimony to this improvement can be obtained by inspecting the performance of even the prematurely halted multi-GPU variant (Chapter~\ref{ch4:experiments-results}). It performs at par with the original pretrained baseline model~\cite{single_view_mpi}, which indicates that the pretrained model successfully began to continue where it left off and specialize in processing video-chat-like frames. We believe it would have run properly if not for the aforementioned resource errors that could potentially point to underlying issues like the possible unchecked growth of TensorFlow graphs per pipeline replica and such. This seems to be the case even though the replicas seem to be getting properly allocated inputs, and their respective outputs seem to be getting well gelled together in the end.

Although the sharpness of the rerendered images is almost twice as good with our chosen model variants as with the pretrained baseline, the predicted MPIs layers have all but collapsed to a single depth layer. This is also evident from how the training would start producing completely gray disparity maps from around step 14000 onward, as noted in chapter~\ref{ch4:experiments-results}. We believe that the reason for this is more likely to be found in the weights we assigned to our various loss functions that aggregate into a mean loss. Ablation experiments involving taking out the pixel loss and/or bringing the smoothness loss way down would help to isolate the issue even more. Furthermore, we believe that if we can crack the reason for some model variants' disparity maps turning gray faster than others (Figure~\ref{fig:gray-disparity-maps}), we will be able to nail the root issue with these reconstructed models of ours. Although we did our best to reconstruct the loss functions and the rest of the training setup as close to the textual descriptions in the paper as possible, it would shine a lot more light on the root cause of the problem if we were able to access the training script of the authors --- something that they have had to keep from the public. Also, since we were also meticulous with our data curation, we do not believe it is likely that the input data has any part to play in the generation of NaN loss errors.

One of the obvious next steps would be to perform \textit{hyperparameter sweeps} with wandb.ai to find optimal hyperparameters, including the weights of the loss functions, and potentially solve the vanishing/exploding gradients problem, which could very well be related to the issue of the swiftly saturating disparity maps. If we can get this plan to work, it would reveal why the pretrained model found it hard to synthesize disparity for video chat frames in the first place --- the model turned out to be not exactly as generalizable as the authors hinted it might be. However, if, after running all possible hyperparameter sweeps with something like wandb.ai, we still find that the model performs poorly, then the apparent next aspect to look at would be the actual training scripts used by the authors to discover how way off the mark we could have been in replicating their network.

\input{chapters/ch5-discussion-conclusion/sec2-conclusion}
\input{chapters/ch5-discussion-conclusion/sec3-future-work}