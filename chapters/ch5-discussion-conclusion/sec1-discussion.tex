\chapter{Discussion}\label{ch:discussion}

% from deep stereo paper the causes for overfitting

% Deep networks have enjoyed huge success in recent
% years, particularly for image understanding tasks [20, 29].
% Despite these successes, relatively little work exists on applying
% deep learning to computer graphics problems and especially
% to generating new views from real imagery. One
% possible reason is the perceived inability of deep networks
% to generate pixels directly, but recent work on denoising
% [35], super-resolution [6], and rendering [21] suggest
% that this is a misconception. Another common objection is
% that deep networks have a huge number of parameters and
% hence are prone to overfitting in the absence of enormous
% quantities of data, but recent work [29] has demonstrated
% state-of-the-art deep networks whose parameters number in
% the low millions, greatly reducing the potential for overfitting.

% from deep stereo on the success of neural nets

% In this work we present a new approach to new view synthesis
% that uses deep networks to regress directly to output
% pixel colors given the posed input images. Our system
% is able to interpolate between views separated by a
% wide baseline and exhibits resilience to traditional failure
% modes, including graceful degradation in the presence of
% scene motion and specularities. We posit this is due to the
% end-to-end nature of the training, and the ability of deep
% networks to learn extremely complex non-linear functions
% of their inputs [25].
% Additionally, although we focus on its application
% to new view problems here, we believe that the
% deep architecture presented can be readily applied to other
% stereo and graphics problems given suitable training data.
% Because
% of the variety of the scenes seen in training our system is robust
% and generalizes to indoor and outdoor imagery, as well
% as to image collections used in prior work.

Through this thesis, we have had the opportunity to simulate both halves of a 2-way pipeline that is able to render novel views from the perspectives of both participants in a video chat. Going by the synthesized monochromatic disparity maps and even by the SSIM values, we found that our model variants struggled to synthesize disparity well. Going by the LPIPS values, we found that they excelled at synthesizing the actual target view itself. This is indeed unexpected owing to the fact that only one of the given 32 MPI layers is able to essentially duplicate the reference image in its entirely. 

Although the sharpness of the rerendered images is almost twice as good with our chosen model variant as with the pretrained baseline, the predicted MPIs layers have all but collapsed to a single depth layer. This is also evident from the way the training would start to produce completely gray disparity maps from around step 14,000 onward, as noted in chapter~\ref{ch4:experiments-results}. We believe that the reason for this is more likely to be found in the weights we assigned to our various loss functions that aggregate into a mean loss. Ablation experiments involving taking out the pixel loss and/or bringing the smoothness loss way down would help to isolate the issue even more. Although we made our best efforts to reconstruct the loss functions and the rest of training setup as close to the textual descriptions in the paper as possible, it would definitely shine a lot more light on the root cause of the problem if we are able to access the training script of the authors --- something that they've had to keep from the public. Also, since we were also meticulous with our data curation, we don't believe it is likely that the input data has any part to play in the generation of NaN loss errors.

One of the obvious next steps would be to perform \textit{hyperparameter sweeps} with wandb.ai to find optimal hyperparameters, including the weights of the loss functions, and potentially solve the vanishing/exploding gradients problem which could very well be related to the issue of the swiftly saturating disparity maps. If we are actually able to get the plan to work, it would reveal why the pretrained model found it hard to synthesize disparity for video chat frames in the first place: it was not exactly as generalizable as the authors hinted it might be. But, if after running all possible hyperparameter sweeps with something like wandb.ai, we still find that the model performs poorly, then the obvious next thing to look at would be the actual training scripts used by the authors to discover how way off the mark we could have been in replicating their network.

\input{chapters/ch5-discussion-conclusion/sec2-conclusion}
\input{chapters/ch5-discussion-conclusion/sec3-future-work}