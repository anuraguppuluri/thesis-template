\section{Future Work}\label{sec:future-work}

We consider exciting future opportunities with this project in this section. We may increase the training speed of the MPI model by making it a multi-GPU model with the constantly-evolving, cutting-edge \texttt{tf.distribute.Strategy} API for distributed training with TensorFlow/Keras. We could perhaps implement taking the average of the head poses of multiple people in the video frames of multiple-participant video conferences rather then just one-on-one video chats and make their average head pose change the rendering viewpoint of the scene to be rerendered. We could proceed to make the pipeline real time by involving a game engine or any other framework capable of further improving real time rendering. We may try training on variable resolution video frames and not all just 720p ones. Overfitting can be further reduced by using a CNN in place of the gradient descent algorithm, similar to Flynn et al.'s DeepView~\cite{flynn_deepview_2019}.

% \url{https://www.pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/}

%  We could perhaps implement taking the average of the head poses of multiple people in the video frames of multiple-participant video conferences rather then just one-on-one video chats and make their average head pose change the rendering viewpoint of the scene to be rerendered. 
% as a toy project but in reality we could do domehting like apple auto focus by clicking on the heads of the multiple participants and let control flow seamlessly.

% Ideal for a headless server?



% possible hypothesis: did cuda support improve disparity map

% possible hypothesis: cuda gpu support is possibly not required for OpenFace inference

% Features of OpenFace like head pose estimation may still work without CUDA recognition by the server either Colab or El Capitan. 

% Need GPU support for maybe MPI training alone and not any other components of the pipeline as the rest of the pipeline is just inference.

% COLMAP will be quicker with GPU as described in \url{https://colmap.github.io/faq.html#available-functionality-without-gpu-cuda}.

% Major hypothesis: one major reason with disparity map to be less because the batch size was only 4 frames at once if multi-gpu access were available then disparity map would have been better/ 

% Mainly mpi and maybe even colmap (for inference speed) seem to require GPU/CUDA support. I've been trying to get GPUs to be used by all my packages on Docker like MPI, COLMAP and their dependencies OpenCV, Dlib etc.
% I doesn't seem to work yet. So I'll resort to using these packages on Docker without GPU support for now. 

% In videos we have recording of my insights today - 8/15/21
% been falling behind and didn't report until resukts 

% and update to flagship versions

% the main thing Dr ventura is that the CUDA install was broken and I needed it for multiple programs like dlib, openface, notwithstanding colmap 

% ask prof ventura to update the nvidia drivers 

% why did i go off on a tangent?
% https://stackoverflow.com/questions/43022843/nvidia-nvml-driver-library-version-mismatch
% Available functionality without GPU/CUDA

% https://colmap.github.io/faq.html#available-functionality-without-gpu-cuda
% If you do not have a CUDA-enabled GPU but some other GPU, you can use all COLMAP functionality except the dense reconstruction part. However, you can use external dense reconstruction software as an alternative, as described in the Tutorial. If you have a GPU with low compute power or you want to execute COLMAP on a machine without an attached display and without CUDA support, you can run all steps on the CPU by specifying the appropriate options (e.g., --SiftExtraction.use_gpu=false for the feature extraction step). But note that this might result in a significant slow-down of the reconstruction pipeline. Please, also note that feature extraction on the CPU can consume excessive RAM for large images in the default settings, which might require manually reducing the maximum image size using --SiftExtraction.max_image_size and/or setting --SiftExtraction.first_octave 0 or by manually limiting the number of threads using --SiftExtraction.num_threads.

% nvidia-smi
% Failed to initialize NVML: Driver/library version mismatch


% what is cuda and nvcc all about?
% https://varhowto.com/check-cuda-version/

% dockerfile colmap run needs to be explained with video clip in MAnnequinChallenge

% proof that gpu is being used by colmap in images in MannequinChallenge


% https://linuxize.com/post/linux-time-command/
% time all functions

% make sure I'm able to restart the model at any poit and continue traning whre I left off and add datasets 

% I need info on inference code 

% Ask ventura why did yoyu say the disparity was bad

% Hopefully we are successfully able to use the latest versions of all components of the MPI pipeline for both training and inference sooner than later.  

% Another application would be if we have a VR headset with a camera on it we can track the rotation of the camera and by doing that you're tracking the rotation of the person's head so that you can render the VR content at the right angle

% Ask prof ventura is colmap autmatically redoes all error videos 
% Ask pro ventura about copyright for his own epipolar geometry lectures

% stereo = 2 images pretty close to each other paired in a special way so that you can get really dense estimations of the depth so basically for every pixel you could get a depth estimate rather than some sparse sampling of keypoints
% canonical stereo case only have pure horizontal translation and no rotation and no other translations in Y or Z  

% blueer values are closer and redder values are farther away in disparity maps

% check 7000 train set and 1400 test set of 2018 paper

% why doesn't 2020 and 2018 papers employ SLAM algorithms directly from COLMAP and not indorectly by themsellves or are they refereing to the same slam algorithms


% i.e., essentially combining 2020 paper with this predecessor 
% chaekc the first chaPpter of interduction of 2019 deep stereo for more info abou this
% As a consequence, the network takes much larger strides along the direction of optimization and converges much sooner and with more accuracy than a network using standard gradient descent.

% Actually, the DeepView paper has a beautiful software to customized, visualize, and render any type of MPI! It's kind of like the state of the art MPI manipulator.~\url{https://augmentedperception.github.io/deepview/}. So maybe improve the 2020 MPI html visualizer upto the standarsd of the deep view one or atleast use it to tweak and experiment the various MPI paraeters lik number of layers etc before deploying and training and testing.

% Adam is better than regular stochastic gradient descent but still not superior to Flynn et al.'s~\cite{flynn_deepview_2019} implementation of learned gradient descent.

This was actually mannequin challenge version 1 hope they get to extend their dataset just like how real estate was able to be extended.