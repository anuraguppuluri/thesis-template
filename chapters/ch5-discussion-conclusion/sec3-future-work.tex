\section{Future Work}\label{sec:future-work}

We consider exciting future opportunities with this project in this section. Right off the bat, we may increase the training speed of the MPI model by making it a successful multi-GPU-utilizing model with the constantly-evolving, cutting-edge \textit{tf.distribute.Strategy} API for distributed training with TensorFlow/Keras. Such a multi-GPU-harnessing model will potentially easily be able to accommodate a batch size of 8 or more, not just 4, which, we believe, would have a positive impact even on overcoming disparity-map-generation inaccuracies. Other beneficial system optimizations include using Docker Multistage Builds~\cite{noauthor_advanced_2020} that allow for harnessing the power of all software components highly efficiently from within a single Dockerfile with multiple \textit{`FROM'} statements such as \textit{FROM tf/tf-gpu-2.2} and \textit{FROM nvidia-cuda10.2-devel-ubuntu18.04}.

Next, we could perhaps implement taking the average of the head poses of multiple people in each video frame of multiple-participant video conferences, not just one-on-one video chats, and making their average head pose change the rendering viewpoint of the scene to be rerendered. Alternatively, perhaps we could do something like a dynamic version of Apple's Autofocus~\cite{noauthor_avcapturedevicefocusmodeautofocus_nodate} wherein we can click/tap on the heads of the multiple participants and make rerendering perspective/focus shift and flow seamlessly. To facilitate these kinds of pipeline enhancements, we could make the pipeline more real-time by involving a game engine or any other framework capable of further improving real-time rendering.

When it comes to model optimizations, overfitting can further be reduced by using a CNN in place of the gradient descent algorithm, similarly to Flynn et al.'s DeepView~\cite{flynn_deepview_2019}. We may incorporate high-intuition-endowing projects like Grad-CAM~\cite{selvaraju_grad-cam_2020} into the wandb.ai~\cite{wandb} pipeline to locate the bottlenecks in the recreated MPI neural net to optimize hyperparameter tuning and produce more accurate results, especially for predicted depths/disparity. Going forward, we could incorporate more recent advancements in single-view MPI view synthesis, such as MINE~\cite{li_mine_2021} and NeX~\cite{wizadwongsa_nex_2021} MPIs. Projects like these are fully open-source and will give us great insights into solving some of our major issues, such as inaccurate depth resolution. To bring the project to a whole another level, we can throw a discriminator component into the mix and turn the model into a Generative Adversarial Network (GAN)~\cite{goodfellow_generative_2014} to possibly produce more extensive and realistic inpainting, and so on. 
	
And last but not least, when it comes to data optimizations, an effective future option may be to consider freezing the first several layers of the pretrained baseline model~\cite{single_view_mpi} that have already learned very well on the (more than) 9 million RealEstate10K~\cite{zhou2018stereo} frames and fine-tune just the remaining few layers on MannequinChallenge~\cite{li2019learning}. This would help since MannequinChallenge is a much smaller dataset than RealEstate10K. Besides, if there is ever going to be a MannequinChallenge version 2, and they get to extend their dataset like how RealEstate10K has been able to, model performance will improve even more when we train on such more-balanced dataset amalgamations. As it is, we may try and improve performance by training on variable resolution video frames, and not just on 720p ones only. Furthermore, as emphasized by Andrew Ng of DeepLearning.AI, Coursera, and Stanford University fame~\cite{bescond_deep-dive_2021}, using advancements in Deep Learning to optimize data curation~\cite{thirumuruganathan_data_2019} is sure to improve any ML model such as ours.